{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"dss-provisioner","text":"<p>Terraform-style resource-as-code for Dataiku DSS</p> <p>Define Dataiku DSS resources as YAML, then plan and apply changes with a familiar CLI workflow.</p> <pre><code># dss-provisioner.yaml\nprovider:\n  host: https://dss.company.com\n  project: ANALYTICS\n\ndatasets:\n  - name: customers_raw\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n\n  - name: customers_clean\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/clean/customers\"\n    managed: true\n    format_type: parquet\n\nrecipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py\n</code></pre> <pre><code>$ dss-provisioner plan\n\n  # dss_snowflake_dataset.customers_raw will be created\n  + resource \"dss_snowflake_dataset\" \"customers_raw\" {\n      + name        = \"customers_raw\"\n      + connection  = \"snowflake_prod\"\n      + schema_name = \"RAW\"\n      + table       = \"CUSTOMERS\"\n    }\n\n  # dss_filesystem_dataset.customers_clean will be created\n  + resource \"dss_filesystem_dataset\" \"customers_clean\" { ... }\n\n  # dss_python_recipe.clean_customers will be created\n  + resource \"dss_python_recipe\" \"clean_customers\" { ... }\n\nPlan: 3 to add, 0 to change, 0 to destroy.\n\n$ dss-provisioner apply --auto-approve\n\n  dss_snowflake_dataset.customers_raw: Creation complete\n  dss_filesystem_dataset.customers_clean: Creation complete\n  dss_python_recipe.clean_customers: Creation complete\n\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\n</code></pre>"},{"location":"#why","title":"Why?","text":"<ul> <li>Version control \u2014 Track pipeline changes in git, review in PRs</li> <li>Reproducibility \u2014 Spin up identical DSS resources across dev/staging/prod</li> <li>Automation \u2014 Deploy DSS changes in CI/CD without UI clicks</li> <li>Visibility \u2014 See exactly what will change before applying</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Quick start \u2014 Get up and running in 5 minutes</li> <li>Architecture \u2014 Understand the plan/apply engine and state model</li> <li>YAML configuration \u2014 Full config reference for all resource types</li> <li>Python API \u2014 Use dss-provisioner as a library</li> <li>CLI reference \u2014 All commands and options</li> <li>API reference \u2014 Auto-generated from source docstrings</li> </ul>"},{"location":"quickstart/","title":"Quick start","text":"<p>Get dss-provisioner running and deploy your first resource in under 5 minutes.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/true-north-partners/dss-provisioner.git\ncd dss-provisioner\nuv sync\n</code></pre> <p>See Installation for more options.</p>"},{"location":"quickstart/#2-set-up-credentials","title":"2. Set up credentials","text":"<pre><code>export DSS_HOST=https://dss.company.com\nexport DSS_API_KEY=your-api-key\n</code></pre> <p>Tip</p> <p>Store credentials in environment variables rather than YAML to avoid committing secrets to version control.</p>"},{"location":"quickstart/#3-create-a-config-file","title":"3. Create a config file","text":"<p>Create <code>dss-provisioner.yaml</code> in your project root:</p> <pre><code>provider:\n  project: MY_PROJECT\n\ndatasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n</code></pre>"},{"location":"quickstart/#4-plan-apply-manage","title":"4. Plan, apply, manage","text":"<pre><code># Preview changes\ndss-provisioner plan\n\n# Apply changes\ndss-provisioner apply\n\n# Apply without confirmation prompt\ndss-provisioner apply --auto-approve\n\n# Detect drift from manual DSS changes\ndss-provisioner drift\n\n# Refresh state from live DSS\ndss-provisioner refresh\n\n# Validate configuration\ndss-provisioner validate\n\n# Tear down all managed resources\ndss-provisioner destroy\n\n# Save plan for later apply\ndss-provisioner plan --out plan.json\ndss-provisioner apply plan.json\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's next?","text":"<ul> <li>Architecture \u2014 understand the plan/apply engine</li> <li>YAML configuration \u2014 full config reference</li> <li>Python API \u2014 use dss-provisioner as a library</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>Understand how dss-provisioner works under the hood.</p> <ul> <li>Architecture \u2014 The plan/apply engine, state model, and dependency graph</li> <li>Resources \u2014 Dataset and recipe resource types, columns, and fields</li> <li>Configuration \u2014 YAML structure, provider settings, and variable substitution</li> </ul>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>dss-provisioner follows Terraform's plan/apply model. You declare desired state in YAML, the engine computes a diff against actual state, and apply executes the changes.</p>"},{"location":"concepts/architecture/#core-loop","title":"Core loop","text":"<pre><code>YAML config \u2500\u2500\u25ba plan() \u2500\u2500\u25ba Plan (diff) \u2500\u2500\u25ba apply() \u2500\u2500\u25ba DSS API calls\n                  \u25b2                                         \u2502\n                  \u2502                                         \u25bc\n              State file \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Updated state\n</code></pre> <ol> <li>Load \u2014 Parse <code>dss-provisioner.yaml</code> into a validated <code>Config</code> object</li> <li>Refresh \u2014 Read live DSS state for each tracked resource, update the state file</li> <li>Plan \u2014 Compare desired resources against state, produce a <code>Plan</code> of <code>ResourceChange</code> items</li> <li>Apply \u2014 Execute the plan in dependency order, updating state after each resource</li> </ol>"},{"location":"concepts/architecture/#key-components","title":"Key components","text":""},{"location":"concepts/architecture/#engine-dssengine","title":"Engine (<code>DSSEngine</code>)","text":"<p>The engine is the central orchestrator. It holds references to the provider, state, and handler registry. Its two main methods are:</p> <ul> <li><code>plan()</code> \u2014 Compares desired resources against current state. Optionally refreshes state first (default: yes). Returns a <code>Plan</code> containing a list of <code>ResourceChange</code> items, each with an action: <code>create</code>, <code>update</code>, <code>delete</code>, or <code>no-op</code>.</li> <li><code>apply()</code> \u2014 Executes a plan. Processes changes in topological order (respecting <code>depends_on</code> and inferred dependencies). Updates the state file after each successful operation. If a resource fails, the error is raised with a partial <code>ApplyResult</code> so you can see what succeeded.</li> </ul>"},{"location":"concepts/architecture/#state-file","title":"State file","text":"<p>The state file (<code>.dss-state.json</code> by default) tracks:</p> <ul> <li>Lineage \u2014 A UUID identifying this state's history, used for stale-plan detection</li> <li>Serial \u2014 Incremented on every write, used for concurrency detection</li> <li>Resources \u2014 A map of resource addresses to their last-known attributes</li> <li>Digest \u2014 SHA256 hash of resource attributes, used to detect plan staleness</li> </ul> <p>State is written atomically (temp file + rename) and a <code>.backup</code> copy is kept.</p>"},{"location":"concepts/architecture/#handlers","title":"Handlers","text":"<p>Handlers implement CRUD operations for each resource type. The engine delegates to the appropriate handler based on resource type. Handler categories:</p> <ul> <li>VariablesHandler \u2014 Reads and writes DSS project variables (singleton per project)</li> <li>ZoneHandler \u2014 Creates, updates, reads, and deletes DSS flow zones</li> <li>GitLibraryHandler \u2014 Creates, updates, reads, and deletes DSS Git library references</li> <li>DatasetHandler \u2014 Creates, updates, reads, and deletes DSS datasets</li> <li>RecipeHandler \u2014 Creates, updates, reads, and deletes DSS recipes</li> <li>ScenarioHandler \u2014 Creates, updates, reads, and deletes DSS scenarios (step-based and Python)</li> </ul>"},{"location":"concepts/architecture/#dependency-graph","title":"Dependency graph","text":"<p>Resources can declare dependencies explicitly via <code>depends_on</code> or implicitly through recipe <code>inputs</code>/<code>outputs</code>. The engine builds a directed acyclic graph and processes resources in topological order during apply.</p> <p>If dependencies contain a cycle, the engine raises <code>DependencyCycleError</code>.</p>"},{"location":"concepts/architecture/#engine-semantics","title":"Engine semantics","text":"<ul> <li>One state file manages one DSS project. Plan/apply will error if the state belongs to a different project (<code>StateProjectMismatchError</code>).</li> <li><code>plan</code> performs a refresh by default \u2014 reads live DSS state and may persist updates. Disable with <code>--no-refresh</code> or <code>refresh=False</code>.</li> <li><code>apply</code> executes changes in dependency order with no rollback. If apply fails, state reflects what was completed. The <code>ApplyError</code> carries a partial <code>ApplyResult</code>.</li> <li>Saved plans (via <code>--out</code>) are checked for staleness via lineage, serial, and state digest before apply.</li> <li>DSS <code>${\u2026}</code> variables (e.g. <code>${projectKey}</code>) are resolved transparently during plan comparison so they don't cause false drift.</li> </ul>"},{"location":"concepts/architecture/#partial-failure","title":"Partial failure","text":"<p>Apply does not support rollback. If a resource fails mid-apply:</p> <ol> <li>All previously applied resources remain in place and are tracked in state</li> <li>The failing resource is not recorded in state</li> <li>An <code>ApplyError</code> is raised with the partial <code>ApplyResult</code> attached</li> <li>Re-running <code>plan</code> + <code>apply</code> will retry only the failed and remaining resources</li> </ol>"},{"location":"concepts/configuration/","title":"Configuration","text":"<p>dss-provisioner uses a single YAML file (<code>dss-provisioner.yaml</code> by default) validated by Pydantic at load time.</p>"},{"location":"concepts/configuration/#config-structure","title":"Config structure","text":"<pre><code>provider:\n  host: https://dss.company.com     # or DSS_HOST env var\n  api_key: secret                   # or DSS_API_KEY env var (recommended)\n  project: MY_PROJECT               # or DSS_PROJECT env var\n\nstate_path: .dss-state.json         # default\n\nzones:\n  - name: ...\n    color: \"#...\"                   # optional hex color\n\ndatasets:\n  - name: ...\n    type: ...\n    # type-specific fields\n\nrecipes:\n  - name: ...\n    type: ...\n    # type-specific fields\n</code></pre>"},{"location":"concepts/configuration/#provider-settings","title":"Provider settings","text":"<p>The <code>provider</code> block configures the DSS connection. All fields support environment variable fallbacks with the <code>DSS_</code> prefix:</p> Field Env var Required Description <code>host</code> <code>DSS_HOST</code> Yes DSS instance URL <code>api_key</code> <code>DSS_API_KEY</code> Yes API key for authentication <code>project</code> <code>DSS_PROJECT</code> Yes Target DSS project key <p>Tip</p> <p>Use environment variables for <code>host</code> and <code>api_key</code> to avoid committing secrets. Omit <code>api_key</code> from YAML entirely and set <code>DSS_API_KEY</code> in the environment instead.</p>"},{"location":"concepts/configuration/#state-path","title":"State path","text":"<p>The <code>state_path</code> field (default: <code>.dss-state.json</code>) controls where the state file is written. This file tracks deployed resources and should be committed to version control for team coordination.</p>"},{"location":"concepts/configuration/#type-discriminators","title":"Type discriminators","text":"<p>Datasets and recipes use the <code>type</code> field as a discriminator for Pydantic's tagged union. Zones have no type discriminator \u2014 there is only one zone type.</p> <pre><code>datasets:\n  - name: my_dataset\n    type: snowflake    # selects SnowflakeDatasetResource\n    # ...\n\nrecipes:\n  - name: my_recipe\n    type: python       # selects PythonRecipeResource\n    # ...\n</code></pre> <p>Available dataset types: <code>snowflake</code>, <code>oracle</code>, <code>filesystem</code>, <code>upload</code>.</p> <p>Available recipe types: <code>python</code>, <code>sql_query</code>, <code>sync</code>.</p>"},{"location":"concepts/configuration/#variable-substitution","title":"Variable substitution","text":"<p>DSS variables like <code>${projectKey}</code> are supported in string fields. They are resolved transparently during plan comparison so they don't cause false drift. For example:</p> <pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"    # resolved to \"MY_PROJECT/raw\" during planning\n</code></pre>"},{"location":"concepts/configuration/#validation","title":"Validation","text":"<p>Run <code>dss-provisioner validate</code> to check your config file without connecting to DSS:</p> <pre><code>$ dss-provisioner validate\nConfiguration is valid.\n\n$ dss-provisioner validate --config custom-config.yaml\nConfiguration is valid.\n</code></pre> <p>Validation catches:</p> <ul> <li>Missing required fields</li> <li>Invalid type discriminators</li> <li>Pydantic type/constraint errors</li> <li>Invalid YAML syntax</li> </ul> <p>See YAML configuration for the full field reference.</p>"},{"location":"concepts/resources/","title":"Resources","text":"<p>Resources are Pydantic models that describe the desired state of a DSS object. They are pure data \u2014 handlers know how to CRUD them.</p>"},{"location":"concepts/resources/#resource-model","title":"Resource model","text":"<p>Every resource has:</p> Field Description <code>name</code> Unique name within the DSS project <code>description</code> Optional description (stored as DSS metadata) <code>tags</code> Optional list of tags <code>depends_on</code> Explicit dependencies on other resources <code>address</code> Computed as <code>{resource_type}.{name}</code> (e.g., <code>dss_filesystem_dataset.raw_data</code>) <p>The <code>address</code> is the primary key used in state tracking and plan output.</p>"},{"location":"concepts/resources/#variables-resource","title":"Variables resource","text":"<p>The <code>variables</code> resource manages DSS project variables \u2014 a singleton key-value store per project. Variables are split into two scopes:</p> <ul> <li><code>standard</code>: shared across all instances</li> <li><code>local</code>: instance-specific overrides</li> </ul> Field Type Default Description <code>name</code> <code>str</code> <code>variables</code> Resource name (singleton \u2014 rarely overridden) <code>standard</code> <code>dict[str, Any]</code> <code>{}</code> Standard project variables <code>local</code> <code>dict[str, Any]</code> <code>{}</code> Local project variables <pre><code>variables:\n  standard:\n    env: prod\n    data_root: /mnt/data\n  local:\n    debug: \"false\"\n</code></pre> <p>Variables use merge/partial semantics: only declared keys are managed. Extra keys in DSS are left alone.</p> <p>Variables have <code>plan_priority: 0</code>, so they are always applied before other resources (zones, datasets, recipes all have priority 100). This ensures variables referenced via <code>${\u2026}</code> in other resources are set before those resources are created.</p>"},{"location":"concepts/resources/#zone-resources","title":"Zone resources","text":"<p>Zones partition a project's flow into logical sections (e.g. raw, curated, reporting). They are provisioned before datasets and recipes so that resources can reference them via the <code>zone</code> field.</p> Field Type Default Description <code>name</code> <code>str</code> \u2014 Zone identifier (used by <code>zone</code> field on datasets/recipes) <code>color</code> <code>str</code> <code>#2ab1ac</code> Hex color displayed in the flow graph <pre><code>zones:\n  - name: raw\n    color: \"#4a90d9\"\n  - name: curated\n    color: \"#7b61ff\"\n</code></pre> <p>Note</p> <p>Flow zones require DSS Enterprise. On Free Edition the zone API returns 404: <code>read</code> and <code>delete</code> degrade gracefully (return None / no-op), while <code>create</code> and <code>update</code> raise a clear <code>RuntimeError</code> since the zone cannot actually be provisioned.</p>"},{"location":"concepts/resources/#git-library-resources","title":"Git library resources","text":"<p>Git libraries import external Git repositories into a project's library, making shared code available to recipes. Each library entry maps to a Git reference in DSS's <code>external-libraries.json</code>.</p> Field Type Default Description <code>name</code> <code>str</code> \u2014 Local target path in the library hierarchy (unique key) <code>repository</code> <code>str</code> \u2014 Git repository URL <code>checkout</code> <code>str</code> <code>main</code> Branch, tag, or commit hash <code>path</code> <code>str</code> <code>\"\"</code> Subpath within the Git repository <code>add_to_python_path</code> <code>bool</code> <code>true</code> Add to <code>pythonPath</code> in <code>external-libraries.json</code> <pre><code>libraries:\n  - name: shared_utils\n    repository: git@github.com:org/dss-shared-lib.git\n    checkout: main\n    path: python\n</code></pre> <p>Libraries have <code>plan_priority: 10</code>, so they are applied after variables (0) but before datasets and recipes (100). This ensures library code is available before recipes that import from it are created.</p> <p>Note</p> <p><code>add_to_python_path</code> is a create-time-only field. Changing it after creation requires deleting and recreating the library. Credentials (SSH keys) are configured at the DSS instance level \u2014 no <code>login</code>/<code>password</code> fields are needed in the YAML config.</p>"},{"location":"concepts/resources/#dataset-resources","title":"Dataset resources","text":"<p>All datasets share common fields from <code>DatasetResource</code>:</p> Field Type Default Description <code>connection</code> <code>str</code> \u2014 DSS connection name <code>managed</code> <code>bool</code> <code>false</code> Whether DSS manages the data lifecycle <code>format_type</code> <code>str</code> \u2014 Storage format (e.g., <code>parquet</code>, <code>csv</code>) <code>format_params</code> <code>dict</code> <code>{}</code> Format-specific parameters <code>columns</code> <code>list[Column]</code> <code>[]</code> Schema columns <code>zone</code> <code>str</code> \u2014 Flow zone (Enterprise only)"},{"location":"concepts/resources/#supported-types","title":"Supported types","text":"Type YAML <code>type</code> Extra required fields Snowflake <code>snowflake</code> <code>connection</code>, <code>schema_name</code>, <code>table</code> Oracle <code>oracle</code> <code>connection</code>, <code>schema_name</code>, <code>table</code> Filesystem <code>filesystem</code> <code>connection</code>, <code>path</code> Upload <code>upload</code> \u2014"},{"location":"concepts/resources/#snowflake-datasets","title":"Snowflake datasets","text":"<pre><code>datasets:\n  - name: my_table\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n    catalog: MY_DB          # optional\n    write_mode: OVERWRITE   # OVERWRITE (default), APPEND, or TRUNCATE\n</code></pre>"},{"location":"concepts/resources/#filesystem-datasets","title":"Filesystem datasets","text":"<pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n    format_type: parquet\n    managed: true\n</code></pre> <p>The <code>path</code> field supports DSS variable substitution \u2014 <code>${projectKey}</code> is resolved transparently during plan comparison.</p>"},{"location":"concepts/resources/#upload-datasets","title":"Upload datasets","text":"<pre><code>datasets:\n  - name: lookup_table\n    type: upload\n    format_type: csv\n    format_params:\n      separator: \",\"\n      charset: utf-8\n</code></pre> <p>Upload datasets are always managed (<code>managed: true</code> by default).</p>"},{"location":"concepts/resources/#recipe-resources","title":"Recipe resources","text":"<p>All recipes share common fields from <code>RecipeResource</code>:</p> Field Type Default Description <code>inputs</code> <code>str \\| list[str]</code> <code>[]</code> Input dataset name(s) <code>outputs</code> <code>str \\| list[str]</code> <code>[]</code> Output dataset name(s) <code>zone</code> <code>str</code> \u2014 Flow zone (Enterprise only) <p>Recipe <code>inputs</code> and <code>outputs</code> create implicit dependencies \u2014 the engine automatically orders recipes after their input datasets.</p>"},{"location":"concepts/resources/#supported-types_1","title":"Supported types","text":"Type YAML <code>type</code> Extra fields Python <code>python</code> <code>code</code> or <code>code_file</code>, <code>code_env</code>, <code>code_wrapper</code> SQL Query <code>sql_query</code> <code>code</code> or <code>code_file</code> Sync <code>sync</code> \u2014"},{"location":"concepts/resources/#python-recipes","title":"Python recipes","text":"<pre><code>recipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py  # loaded at plan time\n    code_env: py311_pandas                   # optional code environment\n</code></pre> <p>You can provide code inline or via <code>code_file</code>. If both are set, <code>code_file</code> takes precedence. The <code>code_wrapper</code> flag controls whether the code runs in DSS's managed I/O wrapper.</p>"},{"location":"concepts/resources/#sql-query-recipes","title":"SQL query recipes","text":"<pre><code>recipes:\n  - name: aggregate_orders\n    type: sql_query\n    inputs: orders_raw\n    outputs: orders_summary\n    code_file: ./recipes/aggregate_orders.sql\n</code></pre>"},{"location":"concepts/resources/#sync-recipes","title":"Sync recipes","text":"<pre><code>recipes:\n  - name: sync_customers\n    type: sync\n    inputs: customers_raw\n    outputs: customers_synced\n</code></pre>"},{"location":"concepts/resources/#scenario-resources","title":"Scenario resources","text":"<p>Scenarios define automated workflows in DSS \u2014 triggers (when to run) and actions (what to do). They are provisioned after datasets and recipes (<code>plan_priority: 200</code>) since scenario steps often reference them.</p> <p>All scenarios share common fields from <code>ScenarioResource</code>:</p> Field Type Default Description <code>active</code> <code>bool</code> <code>true</code> Whether the scenario is enabled <code>triggers</code> <code>list[dict]</code> <code>[]</code> Trigger definitions (temporal, dataset change, etc.)"},{"location":"concepts/resources/#supported-types_2","title":"Supported types","text":"Type YAML <code>type</code> Extra fields Step-based <code>step_based</code> <code>steps</code> Python <code>python</code> <code>code</code> or <code>code_file</code>"},{"location":"concepts/resources/#step-based-scenarios","title":"Step-based scenarios","text":"<pre><code>scenarios:\n  - name: daily_build\n    type: step_based\n    active: true\n    triggers:\n      - type: temporal\n        params:\n          frequency: Daily\n          hour: 2\n          minute: 0\n    steps:\n      - type: build_flowitem\n        name: Build all datasets\n        params:\n          builds:\n            - type: DATASET\n              itemId: my_dataset\n              partitionsSpec: \"\"\n</code></pre>"},{"location":"concepts/resources/#python-scenarios","title":"Python scenarios","text":"<pre><code>scenarios:\n  - name: e2e_test\n    type: python\n    active: false\n    code_file: ./scenarios/e2e_test.py\n</code></pre> <p>You can provide code inline or via <code>code_file</code>. If neither is set, the provisioner looks for <code>scenarios/{name}.py</code> by convention.</p> <p>Note</p> <p>Triggers and steps use a desired-echo strategy: the provisioner stores your declared values and echoes them on read, rather than reading back from DSS. This avoids false drift from auto-generated fields that DSS adds internally.</p>"},{"location":"concepts/resources/#columns","title":"Columns","text":"<p>Define schema columns on datasets:</p> <pre><code>datasets:\n  - name: customers\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/customers\"\n    columns:\n      - name: id\n        type: int\n        description: Customer ID\n      - name: email\n        type: string\n      - name: score\n        type: double\n        meaning: customer_score  # optional DSS meaning\n</code></pre> <p>Supported column types: <code>string</code>, <code>int</code>, <code>bigint</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>date</code>, <code>array</code>, <code>object</code>, <code>map</code>.</p>"},{"location":"guides/","title":"Guides","text":"<p>Step-by-step guides for common tasks.</p> <ul> <li>Installation \u2014 Install from source or set up for development</li> <li>YAML configuration \u2014 Full field reference for all resource types</li> <li>Python API \u2014 Use dss-provisioner programmatically as a library</li> </ul>"},{"location":"guides/installation/","title":"Installation","text":""},{"location":"guides/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or later</li> <li>A running Dataiku DSS instance with API access</li> </ul>"},{"location":"guides/installation/#install-from-source","title":"Install from source","text":"<p>dss-provisioner is not yet published to PyPI. Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre>"},{"location":"guides/installation/#with-uv","title":"With uv","text":"<pre><code>uv pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre>"},{"location":"guides/installation/#development-setup","title":"Development setup","text":"<pre><code>git clone https://github.com/true-north-partners/dss-provisioner.git\ncd dss-provisioner\nuv sync\n</code></pre> <p>This installs the project in editable mode with all development dependencies.</p>"},{"location":"guides/installation/#useful-commands","title":"Useful commands","text":"<pre><code>just test       # Run tests with coverage\njust check      # Lint + format check + type check\njust format     # Auto-format code\njust build      # Build wheel and sdist\njust build_docs # Build documentation\njust serve_docs # Serve documentation locally\n</code></pre>"},{"location":"guides/installation/#verify-installation","title":"Verify installation","text":"<pre><code>$ dss-provisioner --version\ndss-provisioner 0.1.0\n\n$ dss-provisioner --help\n</code></pre>"},{"location":"guides/python-api/","title":"Python API","text":"<p>dss-provisioner can be used as a Python library for programmatic access. All public functions are available from the <code>dss_provisioner.config</code> module.</p>"},{"location":"guides/python-api/#basic-usage","title":"Basic usage","text":"<pre><code>from dss_provisioner.config import load, plan, apply\n\n# Load configuration\nconfig = load(\"dss-provisioner.yaml\")\n\n# Plan changes\np = plan(config)\nprint(f\"Changes: {p.summary()}\")\n\n# Apply changes\nresult = apply(p, config)\nprint(f\"Applied: {result.summary()}\")\n</code></pre>"},{"location":"guides/python-api/#one-step-plan-and-apply","title":"One-step plan and apply","text":"<pre><code>from dss_provisioner.config import load, plan_and_apply\n\nconfig = load(\"dss-provisioner.yaml\")\nresult = plan_and_apply(config)\n</code></pre>"},{"location":"guides/python-api/#drift-detection","title":"Drift detection","text":"<pre><code>from dss_provisioner.config import load, drift\n\nconfig = load(\"dss-provisioner.yaml\")\nchanges = drift(config)\n\nfor change in changes:\n    print(f\"{change.action.value}: {change.address}\")\n    if change.diff:\n        for key, value in change.diff.items():\n            print(f\"  {key}: {value}\")\n</code></pre>"},{"location":"guides/python-api/#refresh-state","title":"Refresh state","text":"<pre><code>from dss_provisioner.config import load, refresh, save_state\n\nconfig = load(\"dss-provisioner.yaml\")\nchanges, new_state = refresh(config)\n\nif changes:\n    print(f\"Found {len(changes)} drifted resources\")\n    save_state(config, new_state)  # persist to disk\n</code></pre>"},{"location":"guides/python-api/#progress-callbacks","title":"Progress callbacks","text":"<p>Track apply progress with a callback:</p> <pre><code>from dss_provisioner.config import load, plan, apply\n\nconfig = load(\"dss-provisioner.yaml\")\np = plan(config)\n\ndef on_progress(change, event):\n    if event == \"start\":\n        print(f\"  {change.address}: applying...\")\n    else:\n        print(f\"  {change.address}: done\")\n\nresult = apply(p, config, progress=on_progress)\n</code></pre>"},{"location":"guides/python-api/#error-handling","title":"Error handling","text":"<pre><code>from dss_provisioner.config import load, plan, apply, ConfigError\nfrom dss_provisioner.engine.errors import (\n    ApplyError,\n    StalePlanError,\n    StateProjectMismatchError,\n)\n\ntry:\n    config = load(\"dss-provisioner.yaml\")\nexcept ConfigError as e:\n    print(f\"Invalid configuration: {e}\")\n    raise\n\np = plan(config)\n\ntry:\n    result = apply(p, config)\nexcept StalePlanError:\n    print(\"State changed since plan was created \u2014 re-plan required\")\nexcept ApplyError as e:\n    print(f\"Apply failed at {e.address}\")\n    print(f\"Successfully applied: {e.result.summary()}\")\n</code></pre>"},{"location":"guides/python-api/#api-reference","title":"API reference","text":"<p>See the full API reference for all classes, functions, and types.</p>"},{"location":"guides/yaml-config/","title":"YAML configuration","text":"<p>Complete reference for the <code>dss-provisioner.yaml</code> configuration file.</p>"},{"location":"guides/yaml-config/#minimal-example","title":"Minimal example","text":"<pre><code>provider:\n  project: MY_PROJECT\n\ndatasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n</code></pre>"},{"location":"guides/yaml-config/#full-example","title":"Full example","text":"<pre><code>provider:\n  host: https://dss.company.com\n  # api_key: omit from YAML \u2014 set DSS_API_KEY env var instead\n  project: ANALYTICS\n\nstate_path: .dss-state.json\n\nvariables:\n  standard:\n    env: prod\n    data_root: /mnt/data\n  local:\n    debug: \"false\"\n\nzones:\n  - name: raw\n    color: \"#4a90d9\"\n  - name: curated\n    color: \"#7b61ff\"\n\nlibraries:\n  - name: shared_utils\n    repository: git@github.com:org/dss-shared-lib.git\n    checkout: main\n    path: python\n\ndatasets:\n  - name: customers_raw\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n    description: Raw customer data from Snowflake\n\n  - name: customers_clean\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/clean/customers\"\n    managed: true\n    format_type: parquet\n    columns:\n      - name: id\n        type: int\n      - name: name\n        type: string\n      - name: email\n        type: string\n    tags:\n      - production\n      - pii\n\nrecipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py\n\n  - name: sync_customers\n    type: sync\n    inputs: customers_clean\n    outputs: customers_synced\n    depends_on:\n      - dss_python_recipe.clean_customers\n\nscenarios:\n  - name: daily_build\n    type: step_based\n    active: true\n    triggers:\n      - type: temporal\n        params:\n          frequency: Daily\n          hour: 2\n          minute: 0\n    steps:\n      - type: build_flowitem\n        name: Build all datasets\n        params:\n          builds:\n            - type: DATASET\n              itemId: customers_clean\n              partitionsSpec: \"\"\n\n  - name: e2e_test\n    type: python\n    active: false\n    code_file: ./scenarios/e2e_test.py\n</code></pre>"},{"location":"guides/yaml-config/#provider","title":"Provider","text":"Field Env var Required Default Description <code>host</code> <code>DSS_HOST</code> Yes \u2014 DSS instance URL <code>api_key</code> <code>DSS_API_KEY</code> Yes \u2014 API key <code>project</code> <code>DSS_PROJECT</code> Yes \u2014 Target project key"},{"location":"guides/yaml-config/#top-level-fields","title":"Top-level fields","text":"Field Type Default Description <code>provider</code> object \u2014 DSS connection settings (required) <code>state_path</code> string <code>.dss-state.json</code> Path to state file <code>variables</code> object \u2014 Project variables (singleton, applied first) <code>zones</code> list <code>[]</code> Flow zone definitions (provisioned before datasets/recipes) <code>libraries</code> list <code>[]</code> Git library references (applied after variables, before datasets/recipes) <code>datasets</code> list <code>[]</code> Dataset resource definitions <code>recipes</code> list <code>[]</code> Recipe resource definitions <code>scenarios</code> list <code>[]</code> Scenario resource definitions (applied after datasets/recipes)"},{"location":"guides/yaml-config/#variables-fields","title":"Variables fields","text":"Field Type Default Description <code>name</code> string <code>variables</code> Resource name (singleton \u2014 rarely overridden) <code>standard</code> dict <code>{}</code> Standard project variables (shared across instances) <code>local</code> dict <code>{}</code> Local project variables (instance-specific) <code>description</code> string <code>\"\"</code> Not used by DSS variables (ignored) <code>tags</code> list <code>[]</code> Not used by DSS variables (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Variables use partial semantics: only declared keys are managed. Extra keys already in DSS are preserved.</p> <p>Variables are always applied before other resource types due to their <code>plan_priority: 0</code> (other resources default to 100).</p>"},{"location":"guides/yaml-config/#zone-fields","title":"Zone fields","text":"Field Type Default Description <code>name</code> string \u2014 Required. Zone identifier (referenced by dataset/recipe <code>zone</code> field) <code>color</code> string <code>#2ab1ac</code> Hex color in <code>#RRGGBB</code> format <code>description</code> string <code>\"\"</code> Not used by DSS zones (ignored) <code>tags</code> list <code>[]</code> Not used by DSS zones (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p>Flow zones require DSS Enterprise. On Free Edition the zone API is unavailable.</p>"},{"location":"guides/yaml-config/#library-fields","title":"Library fields","text":"Field Type Default Description <code>name</code> string \u2014 Required. Local target path in the library hierarchy <code>repository</code> string \u2014 Required. Git repository URL <code>checkout</code> string <code>main</code> Branch, tag, or commit hash to check out <code>path</code> string <code>\"\"</code> Subpath within the Git repository <code>add_to_python_path</code> bool <code>true</code> Add to <code>pythonPath</code> in <code>external-libraries.json</code> <code>description</code> string <code>\"\"</code> Not used by DSS libraries (ignored) <code>tags</code> list <code>[]</code> Not used by DSS libraries (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p><code>add_to_python_path</code> is a create-time-only field. To change it, delete and recreate the library.</p>"},{"location":"guides/yaml-config/#dataset-fields","title":"Dataset fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Dataset name in DSS <code>type</code> string \u2014 Required. One of: <code>snowflake</code>, <code>oracle</code>, <code>filesystem</code>, <code>upload</code> <code>connection</code> string \u2014 DSS connection name <code>managed</code> bool <code>false</code> Whether DSS manages the data lifecycle <code>format_type</code> string \u2014 Storage format (<code>parquet</code>, <code>csv</code>, etc.) <code>format_params</code> dict <code>{}</code> Format-specific parameters <code>columns</code> list <code>[]</code> Schema column definitions <code>zone</code> string \u2014 Flow zone (Enterprise only) <code>description</code> string <code>\"\"</code> Dataset description (metadata) <code>tags</code> list <code>[]</code> DSS tags <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses)"},{"location":"guides/yaml-config/#snowflake-specific-fields","title":"Snowflake-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Snowflake connection name <code>schema_name</code> string \u2014 Required. Snowflake schema <code>table</code> string \u2014 Required. Table name <code>catalog</code> string \u2014 Snowflake database/catalog <code>write_mode</code> string <code>OVERWRITE</code> <code>OVERWRITE</code>, <code>APPEND</code>, or <code>TRUNCATE</code>"},{"location":"guides/yaml-config/#oracle-specific-fields","title":"Oracle-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Oracle connection name <code>schema_name</code> string \u2014 Required. Oracle schema <code>table</code> string \u2014 Required. Table name"},{"location":"guides/yaml-config/#filesystem-specific-fields","title":"Filesystem-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Filesystem connection name <code>path</code> string \u2014 Required. File path (supports <code>${projectKey}</code>)"},{"location":"guides/yaml-config/#upload-specific-fields","title":"Upload-specific fields","text":"<p>Upload datasets have no additional required fields. They default to <code>managed: true</code>.</p>"},{"location":"guides/yaml-config/#recipe-fields","title":"Recipe fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types_1","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Recipe name in DSS <code>type</code> string \u2014 Required. One of: <code>python</code>, <code>sql_query</code>, <code>sync</code> <code>inputs</code> string or list <code>[]</code> Input dataset name(s) <code>outputs</code> string or list <code>[]</code> Output dataset name(s) <code>zone</code> string \u2014 Flow zone (Enterprise only) <code>description</code> string <code>\"\"</code> Recipe description <code>tags</code> list <code>[]</code> DSS tags <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p><code>inputs</code> and <code>outputs</code> accept either a single string or a list of strings. A single string is automatically converted to a one-element list.</p>"},{"location":"guides/yaml-config/#python-specific-fields","title":"Python-specific fields","text":"Field Type Default Description <code>code</code> string <code>\"\"</code> Inline Python code <code>code_file</code> string \u2014 Path to Python file (relative to config file) <code>code_env</code> string \u2014 DSS code environment name <code>code_wrapper</code> bool <code>false</code> Use DSS managed I/O wrapper"},{"location":"guides/yaml-config/#sql-query-specific-fields","title":"SQL query-specific fields","text":"Field Type Default Description <code>code</code> string <code>\"\"</code> Inline SQL code <code>code_file</code> string \u2014 Path to SQL file (relative to config file)"},{"location":"guides/yaml-config/#sync-specific-fields","title":"Sync-specific fields","text":"<p>Sync recipes have no additional fields beyond the common recipe fields.</p>"},{"location":"guides/yaml-config/#scenario-fields","title":"Scenario fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types_2","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Scenario name in DSS <code>type</code> string \u2014 Required. One of: <code>step_based</code>, <code>python</code> <code>active</code> bool <code>true</code> Whether the scenario is enabled <code>triggers</code> list <code>[]</code> Trigger definitions (temporal, dataset change, etc.) <code>description</code> string <code>\"\"</code> Scenario description <code>tags</code> list <code>[]</code> DSS tags <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p>Triggers and steps are stored as raw dicts matching the DSS API format. The provisioner echoes your declared values on read to avoid false drift from auto-generated fields.</p>"},{"location":"guides/yaml-config/#step-based-specific-fields","title":"Step-based-specific fields","text":"Field Type Default Description <code>steps</code> list <code>[]</code> Step definitions (build, run scenario, etc.)"},{"location":"guides/yaml-config/#python-specific-fields_1","title":"Python-specific fields","text":"Field Type Default Description <code>code</code> string <code>\"\"</code> Inline Python code <code>code_file</code> string \u2014 Path to Python file (relative to config file)"},{"location":"guides/yaml-config/#column-definition","title":"Column definition","text":"Field Type Default Description <code>name</code> string \u2014 Required. Column name <code>type</code> string \u2014 Required. One of: <code>string</code>, <code>int</code>, <code>bigint</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>date</code>, <code>array</code>, <code>object</code>, <code>map</code> <code>description</code> string <code>\"\"</code> Column description <code>meaning</code> string \u2014 DSS column meaning"},{"location":"guides/yaml-config/#dependencies","title":"Dependencies","text":"<p>Resources can depend on each other in two ways:</p>"},{"location":"guides/yaml-config/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Use <code>depends_on</code> with full resource addresses:</p> <pre><code>recipes:\n  - name: aggregate\n    type: python\n    depends_on:\n      - dss_python_recipe.clean_data\n</code></pre>"},{"location":"guides/yaml-config/#implicit-dependencies","title":"Implicit dependencies","text":"<p>Recipe <code>inputs</code> and <code>outputs</code> automatically create dependencies on the referenced datasets. You don't need to add <code>depends_on</code> for these.</p> <pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n\nrecipes:\n  - name: process\n    type: python\n    inputs: raw_data      # automatically depends on dss_filesystem_dataset.raw_data\n    outputs: clean_data\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>Detailed reference documentation.</p> <ul> <li>CLI \u2014 All commands, options, and usage examples</li> <li>Resources \u2014 Auto-generated docs for resource classes</li> <li>API \u2014 Auto-generated docs for the full Python API</li> </ul>"},{"location":"reference/api/","title":"API reference","text":"<p>Auto-generated documentation from source code docstrings.</p>"},{"location":"reference/api/#public-api","title":"Public API","text":"<p>The main entry point for programmatic use. All functions are importable from <code>dss_provisioner.config</code>.</p>"},{"location":"reference/api/#dss_provisioner.config","title":"<code>dss_provisioner.config</code>","text":"<p>YAML configuration loading and convenience plan/apply API.</p>"},{"location":"reference/api/#dss_provisioner.config.load","title":"<code>load(path)</code>","text":"<p>Load a YAML configuration file.</p>"},{"location":"reference/api/#dss_provisioner.config.plan","title":"<code>plan(config, *, destroy=False, refresh=True)</code>","text":"<p>Plan changes for the given configuration.</p>"},{"location":"reference/api/#dss_provisioner.config.apply","title":"<code>apply(plan_obj, config, *, progress=None)</code>","text":"<p>Apply a previously computed plan.</p>"},{"location":"reference/api/#dss_provisioner.config.plan_and_apply","title":"<code>plan_and_apply(config, *, destroy=False, refresh=True)</code>","text":"<p>Plan and apply in one step.</p>"},{"location":"reference/api/#dss_provisioner.config.refresh","title":"<code>refresh(config)</code>","text":"<p>Refresh state from the live DSS instance (not persisted).</p> <p>Returns the list of drift changes and the new state. Call :func:<code>save_state</code> to persist the returned state to disk.</p>"},{"location":"reference/api/#dss_provisioner.config.save_state","title":"<code>save_state(config, state)</code>","text":"<p>Persist state to disk.</p>"},{"location":"reference/api/#dss_provisioner.config.drift","title":"<code>drift(config)</code>","text":"<p>Detect drift between state file and live DSS.</p>"},{"location":"reference/api/#configuration","title":"Configuration","text":""},{"location":"reference/api/#config","title":"Config","text":""},{"location":"reference/api/#dss_provisioner.config.schema.Config","title":"<code>dss_provisioner.config.schema.Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Provisioning configuration \u2014 validates YAML structure directly.</p>"},{"location":"reference/api/#dss_provisioner.config.schema.Config.resources","title":"<code>resources</code>  <code>property</code>","text":"<p>All declared resources \u2014 ordering is not significant.</p>"},{"location":"reference/api/#providerconfig","title":"ProviderConfig","text":""},{"location":"reference/api/#dss_provisioner.config.schema.ProviderConfig","title":"<code>dss_provisioner.config.schema.ProviderConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>DSS provider connection settings.</p> <p>Fields can be set via YAML (constructor kwargs) or environment variables with the <code>DSS_</code> prefix.  Constructor kwargs take precedence.</p> <p><code>api_key</code> is typically provided via the <code>DSS_API_KEY</code> environment variable rather than YAML to avoid committing secrets to version control.</p>"},{"location":"reference/api/#engine-types","title":"Engine types","text":""},{"location":"reference/api/#plan","title":"Plan","text":""},{"location":"reference/api/#dss_provisioner.engine.types.Plan","title":"<code>dss_provisioner.engine.types.Plan</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#resourcechange","title":"ResourceChange","text":""},{"location":"reference/api/#dss_provisioner.engine.types.ResourceChange","title":"<code>dss_provisioner.engine.types.ResourceChange</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#action","title":"Action","text":""},{"location":"reference/api/#dss_provisioner.engine.types.Action","title":"<code>dss_provisioner.engine.types.Action</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"reference/api/#applyresult","title":"ApplyResult","text":""},{"location":"reference/api/#dss_provisioner.engine.types.ApplyResult","title":"<code>dss_provisioner.engine.types.ApplyResult</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#planmetadata","title":"PlanMetadata","text":""},{"location":"reference/api/#dss_provisioner.engine.types.PlanMetadata","title":"<code>dss_provisioner.engine.types.PlanMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#engine","title":"Engine","text":""},{"location":"reference/api/#dssengine","title":"DSSEngine","text":""},{"location":"reference/api/#dss_provisioner.engine.engine.DSSEngine","title":"<code>dss_provisioner.engine.engine.DSSEngine(*, provider, project_key, state_path, registry)</code>","text":"<p>Terraform-like plan/apply engine for DSS resources.</p>"},{"location":"reference/api/#dss_provisioner.engine.engine.DSSEngine.refresh","title":"<code>refresh(*, persist=False)</code>","text":"<p>Refresh state from DSS. Returns (pre_refresh, post_refresh).</p>"},{"location":"reference/api/#state","title":"State","text":""},{"location":"reference/api/#state_1","title":"State","text":""},{"location":"reference/api/#dss_provisioner.core.state.State","title":"<code>dss_provisioner.core.state.State</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Terraform-style state file for tracking deployed resources.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>State file format version</p> <code>project_key</code> <code>str</code> <p>DSS project key</p> <code>resources</code> <code>dict[str, ResourceInstance]</code> <p>Mapping of resource addresses to instances</p> <code>outputs</code> <code>dict[str, Any]</code> <p>Output values from the configuration</p>"},{"location":"reference/api/#dss_provisioner.core.state.State.save","title":"<code>save(path)</code>","text":"<p>Save state to a JSON file.</p> <ul> <li>Writes atomically (temp file + rename)</li> <li>Writes a <code>.backup</code> copy of the previous state when overwriting</li> </ul>"},{"location":"reference/api/#dss_provisioner.core.state.State.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load state from a JSON file.</p>"},{"location":"reference/api/#dss_provisioner.core.state.State.load_or_create","title":"<code>load_or_create(path, project_key)</code>  <code>classmethod</code>","text":"<p>Load existing state or create a new one.</p>"},{"location":"reference/api/#resourceinstance","title":"ResourceInstance","text":""},{"location":"reference/api/#dss_provisioner.core.state.ResourceInstance","title":"<code>dss_provisioner.core.state.ResourceInstance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A tracked resource instance in the state file.</p> <p>Attributes:</p> Name Type Description <code>address</code> <code>str</code> <p>Unique resource address (e.g., \"dss_recipe.join_orders\")</p> <code>resource_type</code> <code>str</code> <p>Type of the resource (e.g., \"dss_join_recipe\")</p> <code>name</code> <code>str</code> <p>Resource name (e.g., \"join_orders\")</p> <code>attributes</code> <code>dict[str, Any]</code> <p>Current attribute values</p> <code>attributes_hash</code> <code>str</code> <p>SHA256 hash for change detection</p> <code>dependencies</code> <code>list[str]</code> <p>Addresses of dependencies</p> <code>created_at</code> <code>datetime</code> <p>When the resource was created</p> <code>updated_at</code> <code>datetime</code> <p>When the resource was last updated</p>"},{"location":"reference/api/#provider","title":"Provider","text":""},{"location":"reference/api/#dssprovider","title":"DSSProvider","text":""},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider","title":"<code>dss_provisioner.core.provider.DSSProvider</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Connection configuration for a DSS instance.</p> <p>For external use, provide host and auth. For internal use (inside DSS notebooks/recipes), use the <code>from_client</code> classmethod to inject a client.</p> <p>Examples:</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider--external-with-api-key","title":"External with API key","text":"<p>provider = DSSProvider(     host=\"https://dss.company.com\",     auth=ApiKeyAuth(api_key=\"my-api-key\"), )</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider--inside-dss-notebook","title":"Inside DSS notebook","text":"<p>import dataiku provider = DSSProvider.from_client(dataiku.api_client())</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.client","title":"<code>client</code>  <code>cached</code> <code>property</code>","text":"<p>Get the DSS client.</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.from_client","title":"<code>from_client(client)</code>  <code>classmethod</code>","text":"<p>Create a provider with an injected client.</p> <p>Use this for running inside DSS or for testing with a mock client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DSSClient</code> <p>A pre-configured DSSClient instance</p> required Example <p>import dataiku provider = DSSProvider.from_client(dataiku.api_client())</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.in_project","title":"<code>in_project(project_key)</code>","text":"<p>Bind this provider to a single project for convenience.</p>"},{"location":"reference/api/#apikeyauth","title":"ApiKeyAuth","text":""},{"location":"reference/api/#dss_provisioner.core.provider.ApiKeyAuth","title":"<code>dss_provisioner.core.provider.ApiKeyAuth</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>API key authentication for DSS.</p>"},{"location":"reference/api/#errors","title":"Errors","text":""},{"location":"reference/api/#dss_provisioner.engine.errors","title":"<code>dss_provisioner.engine.errors</code>","text":"<p>Engine error types.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for engine errors.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.UnknownResourceTypeError","title":"<code>UnknownResourceTypeError(resource_type)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when a resource type has no registration/handler.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.DuplicateAddressError","title":"<code>DuplicateAddressError(address)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when multiple desired resources share the same address.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.DependencyCycleError","title":"<code>DependencyCycleError(addresses)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when dependencies contain a cycle.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StateProjectMismatchError","title":"<code>StateProjectMismatchError(expected, got)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when the on-disk state belongs to a different project.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StalePlanError","title":"<code>StalePlanError</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when applying a plan against a different state than planned.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StateLockError","title":"<code>StateLockError</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when the state lock cannot be acquired or released.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ValidationError","title":"<code>ValidationError(errors)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>One or more resources failed plan validation.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ApplyError","title":"<code>ApplyError(*, applied, address, message)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when an apply fails mid-way through.</p> <p>Carries the partial result (what was applied before the failure) so callers can inspect progress.  The original exception is chained via <code>__cause__</code>.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ApplyCanceled","title":"<code>ApplyCanceled</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when an apply is canceled (e.g., Ctrl-C).</p>"},{"location":"reference/cli/","title":"CLI reference","text":"<p>dss-provisioner provides a Typer-based CLI with six commands.</p>"},{"location":"reference/cli/#global-options","title":"Global options","text":"<p>All commands accept:</p> Option Default Description <code>--config</code> <code>dss-provisioner.yaml</code> Path to configuration file <code>--no-color</code> <code>false</code> Disable colored output"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#plan","title":"<code>plan</code>","text":"<p>Show changes required by the current configuration.</p> <pre><code>dss-provisioner plan [OPTIONS]\n</code></pre> Option Default Description <code>--out PATH</code> \u2014 Save plan to a JSON file for later <code>apply</code> <code>--no-refresh</code> <code>false</code> Skip refreshing state from DSS before planning <p>By default, <code>plan</code> refreshes state from the live DSS instance before computing the diff. Use <code>--no-refresh</code> to plan against the local state file only.</p>"},{"location":"reference/cli/#apply","title":"<code>apply</code>","text":"<p>Apply the changes required by the current configuration.</p> <pre><code>dss-provisioner apply [PLAN_FILE] [OPTIONS]\n</code></pre> Option Default Description <code>PLAN_FILE</code> \u2014 Path to a saved plan (from <code>plan --out</code>) <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <code>--no-refresh</code> <code>false</code> Skip refreshing state before planning <p>If <code>PLAN_FILE</code> is provided, apply uses the saved plan (checking for staleness). Otherwise, it runs <code>plan</code> + <code>apply</code> in one step.</p>"},{"location":"reference/cli/#destroy","title":"<code>destroy</code>","text":"<p>Destroy all managed resources.</p> <pre><code>dss-provisioner destroy [OPTIONS]\n</code></pre> Option Default Description <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <p>Plans deletion of all resources tracked in the state file, then applies in reverse dependency order.</p>"},{"location":"reference/cli/#refresh","title":"<code>refresh</code>","text":"<p>Refresh state from the live DSS instance.</p> <pre><code>dss-provisioner refresh [OPTIONS]\n</code></pre> Option Default Description <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <p>Reads the current state of each tracked resource from DSS and updates the local state file. Useful after manual changes in the DSS UI.</p>"},{"location":"reference/cli/#drift","title":"<code>drift</code>","text":"<p>Show drift between state and the live DSS instance.</p> <pre><code>dss-provisioner drift [OPTIONS]\n</code></pre> <p>Read-only command that compares the state file against live DSS. Does not modify state. Shows which resources have drifted and what changed.</p>"},{"location":"reference/cli/#validate","title":"<code>validate</code>","text":"<p>Validate the configuration file.</p> <pre><code>dss-provisioner validate [OPTIONS]\n</code></pre> <p>Parses and validates the YAML configuration without connecting to DSS. Useful in CI pipelines or pre-commit hooks.</p>"},{"location":"reference/resources/","title":"Resource reference","text":"<p>Auto-generated API documentation for all resource types.</p>"},{"location":"reference/resources/#base","title":"Base","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource","title":"<code>dss_provisioner.resources.base.Resource</code>","text":"<p>Base class for all DSS resources.</p> <p>Resources are pure data - they define the desired state. Handlers know how to CRUD resources.</p>"},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.description","title":"<code>description = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.tags","title":"<code>tags = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.depends_on","title":"<code>depends_on = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.address","title":"<code>address</code>  <code>property</code>","text":"<p>Unique address for this resource (e.g., 'dss_dataset.my_dataset').</p>"},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.reference_names","title":"<code>reference_names()</code>","text":"<p>Names of other resources this one references (auto-collected from Ref markers).</p>"},{"location":"reference/resources/#variables","title":"Variables","text":""},{"location":"reference/resources/#variablesresource","title":"VariablesResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.variables.VariablesResource","title":"<code>dss_provisioner.resources.variables.VariablesResource</code>","text":"<p>Project variables resource (singleton per project).</p> <p>Manages DSS project variables in two scopes:</p> <ul> <li><code>standard</code>: shared across all instances</li> <li><code>local</code>: instance-specific overrides</li> </ul>"},{"location":"reference/resources/#zones","title":"Zones","text":""},{"location":"reference/resources/#zoneresource","title":"ZoneResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.zone.ZoneResource","title":"<code>dss_provisioner.resources.zone.ZoneResource</code>","text":"<p>A DSS flow zone.</p> <p>Zones partition the flow graph into logical sections (e.g. raw, curated). They must be provisioned before datasets/recipes that reference them.</p>"},{"location":"reference/resources/#git-libraries","title":"Git Libraries","text":""},{"location":"reference/resources/#gitlibraryresource","title":"GitLibraryResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.git_library.GitLibraryResource","title":"<code>dss_provisioner.resources.git_library.GitLibraryResource</code>","text":"<p>Git library reference for a DSS project.</p> <p>Manages external Git repositories imported into the project library. Each entry maps to a Git reference in DSS's <code>external-libraries.json</code>.</p>"},{"location":"reference/resources/#datasets","title":"Datasets","text":""},{"location":"reference/resources/#datasetresource","title":"DatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.DatasetResource","title":"<code>dss_provisioner.resources.dataset.DatasetResource</code>","text":"<p>Base resource for DSS datasets.</p>"},{"location":"reference/resources/#dss_provisioner.resources.dataset.DatasetResource.to_dss_params","title":"<code>to_dss_params()</code>","text":"<p>Build the DSS API params dict from DSSParam-annotated fields.</p>"},{"location":"reference/resources/#snowflakedatasetresource","title":"SnowflakeDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.SnowflakeDatasetResource","title":"<code>dss_provisioner.resources.dataset.SnowflakeDatasetResource</code>","text":"<p>Snowflake-specific dataset resource.</p>"},{"location":"reference/resources/#oracledatasetresource","title":"OracleDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.OracleDatasetResource","title":"<code>dss_provisioner.resources.dataset.OracleDatasetResource</code>","text":"<p>Oracle-specific dataset resource.</p>"},{"location":"reference/resources/#filesystemdatasetresource","title":"FilesystemDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.FilesystemDatasetResource","title":"<code>dss_provisioner.resources.dataset.FilesystemDatasetResource</code>","text":"<p>Filesystem-specific dataset resource.</p>"},{"location":"reference/resources/#uploaddatasetresource","title":"UploadDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.UploadDatasetResource","title":"<code>dss_provisioner.resources.dataset.UploadDatasetResource</code>","text":"<p>Upload-specific dataset resource.</p>"},{"location":"reference/resources/#column","title":"Column","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.Column","title":"<code>dss_provisioner.resources.dataset.Column</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A column in a dataset schema.</p>"},{"location":"reference/resources/#recipes","title":"Recipes","text":""},{"location":"reference/resources/#reciperesource","title":"RecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.RecipeResource","title":"<code>dss_provisioner.resources.recipe.RecipeResource</code>","text":"<p>Base resource for DSS recipes.</p>"},{"location":"reference/resources/#pythonreciperesource","title":"PythonRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.PythonRecipeResource","title":"<code>dss_provisioner.resources.recipe.PythonRecipeResource</code>","text":"<p>Python recipe resource.</p>"},{"location":"reference/resources/#sqlqueryreciperesource","title":"SQLQueryRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.SQLQueryRecipeResource","title":"<code>dss_provisioner.resources.recipe.SQLQueryRecipeResource</code>","text":"<p>SQL query recipe resource.</p>"},{"location":"reference/resources/#syncreciperesource","title":"SyncRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.SyncRecipeResource","title":"<code>dss_provisioner.resources.recipe.SyncRecipeResource</code>","text":"<p>Sync recipe resource.</p>"},{"location":"reference/resources/#scenarios","title":"Scenarios","text":""},{"location":"reference/resources/#scenarioresource","title":"ScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.ScenarioResource","title":"<code>dss_provisioner.resources.scenario.ScenarioResource</code>","text":"<p>Base resource for DSS scenarios.</p>"},{"location":"reference/resources/#stepbasedscenarioresource","title":"StepBasedScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.StepBasedScenarioResource","title":"<code>dss_provisioner.resources.scenario.StepBasedScenarioResource</code>","text":"<p>Step-based scenario with declarative steps.</p>"},{"location":"reference/resources/#pythonscenarioresource","title":"PythonScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.PythonScenarioResource","title":"<code>dss_provisioner.resources.scenario.PythonScenarioResource</code>","text":"<p>Custom Python scenario.</p>"}]}