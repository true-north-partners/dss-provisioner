{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"dss-provisioner","text":"<p>Terraform-style resource-as-code for Dataiku DSS</p> <p>Define Dataiku DSS resources as YAML, then plan and apply changes with a familiar CLI workflow.</p> <pre><code># dss-provisioner.yaml\nprovider:\n  host: https://dss.company.com\n  project: ANALYTICS\n\ndatasets:\n  - name: customers_raw\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n\n  - name: customers_clean\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/clean/customers\"\n    managed: true\n    format_type: parquet\n\nrecipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py\n</code></pre> <pre><code>$ dss-provisioner plan\n\n  # dss_snowflake_dataset.customers_raw will be created\n  + resource \"dss_snowflake_dataset\" \"customers_raw\" {\n      + name        = \"customers_raw\"\n      + connection  = \"snowflake_prod\"\n      + schema_name = \"RAW\"\n      + table       = \"CUSTOMERS\"\n    }\n\n  # dss_filesystem_dataset.customers_clean will be created\n  + resource \"dss_filesystem_dataset\" \"customers_clean\" { ... }\n\n  # dss_python_recipe.clean_customers will be created\n  + resource \"dss_python_recipe\" \"clean_customers\" { ... }\n\nPlan: 3 to add, 0 to change, 0 to destroy.\n\n$ dss-provisioner apply --auto-approve\n\n  dss_snowflake_dataset.customers_raw: Creation complete\n  dss_filesystem_dataset.customers_clean: Creation complete\n  dss_python_recipe.clean_customers: Creation complete\n\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\n</code></pre>"},{"location":"#why","title":"Why?","text":"<ul> <li>Version control \u2014 Track pipeline changes in git, review in PRs</li> <li>Reproducibility \u2014 Spin up identical DSS resources across dev/staging/prod</li> <li>Automation \u2014 Deploy DSS changes in CI/CD without UI clicks</li> <li>Visibility \u2014 See exactly what will change before applying</li> <li>Composability \u2014 Write Python modules that generate resources from parameters</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Quick start \u2014 Get up and running in 5 minutes</li> <li>End-to-end examples \u2014 Choose canonical Free/Enterprise and Python API examples</li> <li>Architecture \u2014 Understand the plan/apply engine and state model</li> <li>YAML configuration \u2014 Full config reference for all resource types</li> <li>Writing modules \u2014 Create reusable resource generators in Python</li> <li>Python API \u2014 Use dss-provisioner as a library</li> <li>CLI reference \u2014 All commands and options</li> <li>API reference \u2014 Auto-generated from source docstrings</li> </ul>"},{"location":"quickstart/","title":"Quick start","text":"<p>Get dss-provisioner running and deploy your first resource in under 5 minutes.</p>"},{"location":"quickstart/#1-install","title":"1. Install","text":"<pre><code>pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/true-north-partners/dss-provisioner.git\ncd dss-provisioner\nuv sync\n</code></pre> <p>See Installation for more options.</p>"},{"location":"quickstart/#2-set-up-credentials","title":"2. Set up credentials","text":"<pre><code>export DSS_HOST=https://dss.company.com\nexport DSS_API_KEY=your-api-key\n</code></pre> <p>Tip</p> <p>Store credentials in environment variables rather than YAML to avoid committing secrets to version control.</p>"},{"location":"quickstart/#3-create-a-config-file","title":"3. Create a config file","text":"<p>Create <code>dss-provisioner.yaml</code> in your project root:</p> <pre><code>provider:\n  project: MY_PROJECT\n\ndatasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n</code></pre>"},{"location":"quickstart/#4-plan-apply-manage","title":"4. Plan, apply, manage","text":"<pre><code># Preview changes\ndss-provisioner plan\n\n# Apply changes\ndss-provisioner apply\n\n# Apply without confirmation prompt\ndss-provisioner apply --auto-approve\n\n# Detect drift from manual DSS changes\ndss-provisioner drift\n\n# Refresh state from live DSS\ndss-provisioner refresh\n\n# Validate configuration\ndss-provisioner validate\n\n# Tear down all managed resources\ndss-provisioner destroy\n\n# Save plan for later apply\ndss-provisioner plan --out plan.json\ndss-provisioner apply plan.json\n</code></pre>"},{"location":"quickstart/#whats-next","title":"What's next?","text":"<ul> <li>Architecture \u2014 understand the plan/apply engine</li> <li>End-to-end examples \u2014 pick a canonical Free or Enterprise starter</li> <li>YAML configuration \u2014 full config reference</li> <li>Writing modules \u2014 create reusable resource generators in Python</li> <li>Python API \u2014 use dss-provisioner as a library</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>Understand how dss-provisioner works under the hood.</p> <ul> <li>Architecture \u2014 The plan/apply engine, state model, and dependency graph</li> <li>Resources \u2014 Dataset and recipe resource types, columns, and fields</li> <li>Configuration \u2014 YAML structure, provider settings, and variable substitution</li> </ul>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>dss-provisioner follows Terraform's plan/apply model. You declare desired state in YAML, the engine computes a diff against actual state, and apply executes the changes.</p>"},{"location":"concepts/architecture/#core-loop","title":"Core loop","text":"<pre><code>YAML config \u2500\u2500\u25ba plan() \u2500\u2500\u25ba Plan (diff) \u2500\u2500\u25ba apply() \u2500\u2500\u25ba DSS API calls\n                  \u25b2                                         \u2502\n                  \u2502                                         \u25bc\n              State file \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Updated state\n</code></pre> <ol> <li>Load \u2014 Parse <code>dss-provisioner.yaml</code> into a validated <code>Config</code> object. If <code>modules:</code> are defined, expand them into resources at this stage</li> <li>Refresh \u2014 Read live DSS state for each tracked resource, update the state file</li> <li>Plan \u2014 Compare desired resources against state, produce a <code>Plan</code> of <code>ResourceChange</code> items</li> <li>Apply \u2014 Execute the plan in dependency order, updating state after each resource</li> </ol>"},{"location":"concepts/architecture/#key-components","title":"Key components","text":""},{"location":"concepts/architecture/#engine-dssengine","title":"Engine (<code>DSSEngine</code>)","text":"<p>The engine is the central orchestrator. It holds references to the provider, state, and handler registry. Its two main methods are:</p> <ul> <li><code>plan()</code> \u2014 Compares desired resources against current state. Optionally refreshes state first (default: yes). Runs a validation pass that checks <code>depends_on</code> references and <code>zone</code> references before computing the diff. Returns a <code>Plan</code> containing a list of <code>ResourceChange</code> items, each with an action: <code>create</code>, <code>update</code>, <code>delete</code>, or <code>no-op</code>.</li> <li><code>apply()</code> \u2014 Executes a plan. Processes changes in topological order (respecting <code>depends_on</code> and inferred dependencies). Updates the state file after each successful operation. If a resource fails, the error is raised with a partial <code>ApplyResult</code> so you can see what succeeded.</li> </ul>"},{"location":"concepts/architecture/#state-file","title":"State file","text":"<p>The state file (<code>.dss-state.json</code> by default) tracks:</p> <ul> <li>Lineage \u2014 A UUID identifying this state's history, used for stale-plan detection</li> <li>Serial \u2014 Incremented on every write, used for concurrency detection</li> <li>Resources \u2014 A map of resource addresses to their last-known attributes</li> <li>Digest \u2014 SHA256 hash of resource attributes, used to detect plan staleness</li> </ul> <p>State is written atomically (temp file + rename) and a <code>.backup</code> copy is kept.</p>"},{"location":"concepts/architecture/#handlers","title":"Handlers","text":"<p>Handlers implement CRUD operations for each resource type. The engine delegates to the appropriate handler based on resource type. Handler categories:</p> <ul> <li>VariablesHandler \u2014 Reads and writes DSS project variables (singleton per project)</li> <li>ZoneHandler \u2014 Creates, updates, reads, and deletes DSS flow zones</li> <li>GitLibraryHandler \u2014 Creates, updates, reads, and deletes DSS Git library references</li> <li>DatasetHandler \u2014 Creates, updates, reads, and deletes DSS datasets</li> <li>ExposedObjectHandler \u2014 Manages project-level exposed object rules (<code>exposedObjects</code>)</li> <li>ForeignHandler \u2014 Declares/validates cross-project foreign dataset/folder references</li> <li>RecipeHandler \u2014 Creates, updates, reads, and deletes DSS recipes</li> <li>ScenarioHandler \u2014 Creates, updates, reads, and deletes DSS scenarios (step-based and Python)</li> </ul>"},{"location":"concepts/architecture/#dependency-graph","title":"Dependency graph","text":"<p>Resources can declare dependencies explicitly via <code>depends_on</code> or implicitly through recipe <code>inputs</code>/<code>outputs</code>. The engine builds a directed acyclic graph and processes resources in topological order during apply.</p> <p>For cross-project aliases, the planner normalizes recipe refs to DSS full refs (<code>SOURCE_PROJECT.object</code>) while preserving dependency edges from declared aliases to foreign resources.</p> <p>If dependencies contain a cycle, the engine raises <code>DependencyCycleError</code>.</p>"},{"location":"concepts/architecture/#engine-semantics","title":"Engine semantics","text":"<ul> <li>One state file manages one DSS project. Plan/apply will error if the state belongs to a different project (<code>StateProjectMismatchError</code>).</li> <li><code>plan</code> performs a refresh by default \u2014 reads live DSS state and may persist updates. Disable with <code>--no-refresh</code> or <code>refresh=False</code>.</li> <li><code>apply</code> executes changes in dependency order with no rollback. If apply fails, state reflects what was completed. The <code>ApplyError</code> carries a partial <code>ApplyResult</code>.</li> <li>Saved plans (via <code>--out</code>) are checked for staleness via lineage, serial, and state digest before apply.</li> <li>DSS <code>${\u2026}</code> variables (e.g. <code>${projectKey}</code>) are resolved transparently during plan comparison so they don't cause false drift.</li> </ul>"},{"location":"concepts/architecture/#preview-workflow","title":"Preview workflow","text":"<p>The <code>preview</code> CLI command composes an ephemeral config overlay and then reuses the normal plan/apply engine:</p> <ol> <li>Resolve current branch (<code>--branch</code> can override auto-detection)</li> <li>Derive preview project key from base project + branch</li> <li>Derive preview state path from base <code>state_path</code> (for isolation)</li> <li>Rewrite library entries with <code>repository: self</code> to the local git <code>origin</code> URL and set checkout to the branch</li> <li>Create (or reuse) the preview DSS project, then run standard <code>plan()</code> + <code>apply()</code> against it</li> </ol> <p>This keeps the engine unchanged while isolating preview state and project scope.</p>"},{"location":"concepts/architecture/#partial-failure","title":"Partial failure","text":"<p>Apply does not support rollback. If a resource fails mid-apply:</p> <ol> <li>All previously applied resources remain in place and are tracked in state</li> <li>The failing resource is not recorded in state</li> <li>An <code>ApplyError</code> is raised with the partial <code>ApplyResult</code> attached</li> <li>Re-running <code>plan</code> + <code>apply</code> will retry only the failed and remaining resources</li> </ol>"},{"location":"concepts/configuration/","title":"Configuration","text":"<p>dss-provisioner uses a single YAML file (<code>dss-provisioner.yaml</code> by default) validated by Pydantic at load time.</p>"},{"location":"concepts/configuration/#config-structure","title":"Config structure","text":"<pre><code>provider:\n  host: https://dss.company.com     # or DSS_HOST env var\n  api_key: secret                   # or DSS_API_KEY env var (recommended)\n  project: MY_PROJECT               # or DSS_PROJECT env var\n\nstate_path: .dss-state.json         # default\n\nzones:\n  - name: ...\n    color: \"#...\"                   # optional hex color\n\ndatasets:\n  - name: ...\n    type: ...\n    # type-specific fields\n\nexposed_objects:\n  - name: ...\n    type: dataset|managed_folder\n    target_projects: [...]\n\nforeign_datasets:\n  - name: ...\n    source_project: ...\n    source_name: ...\n\nforeign_managed_folders:\n  - name: ...\n    source_project: ...\n    source_name: ...\n\nrecipes:\n  - name: ...\n    type: ...\n    # type-specific fields\n</code></pre>"},{"location":"concepts/configuration/#provider-settings","title":"Provider settings","text":"<p>The <code>provider</code> block configures the DSS connection. All fields support environment variable fallbacks with the <code>DSS_</code> prefix:</p> Field Env var Required Default Description <code>host</code> <code>DSS_HOST</code> Yes \u2014 DSS instance URL <code>api_key</code> <code>DSS_API_KEY</code> Yes \u2014 API key for authentication <code>project</code> <code>DSS_PROJECT</code> Yes \u2014 Target DSS project key <code>verify_ssl</code> <code>DSS_VERIFY_SSL</code> No <code>true</code> Verify SSL certificates. Set <code>false</code> for self-signed certs <p>Tip</p> <p>Use environment variables for <code>host</code> and <code>api_key</code> to avoid committing secrets. Omit <code>api_key</code> from YAML entirely and set <code>DSS_API_KEY</code> in the environment instead.</p> <p>A <code>.env</code> file next to the config file is loaded automatically. This is convenient for local development:</p> <pre><code># .env\nDSS_HOST=http://localhost:11200\nDSS_API_KEY=your-api-key\n</code></pre> <p>Priority (highest wins): YAML value &gt; shell environment variable &gt; <code>.env</code> file &gt; default.</p>"},{"location":"concepts/configuration/#state-path","title":"State path","text":"<p>The <code>state_path</code> field (default: <code>.dss-state.json</code>) controls where the state file is written. This file tracks deployed resources and should be committed to version control for team coordination.</p>"},{"location":"concepts/configuration/#type-discriminators","title":"Type discriminators","text":"<p>Datasets and recipes use the <code>type</code> field as a discriminator for Pydantic's tagged union. Zones have no type discriminator \u2014 there is only one zone type.</p> <pre><code>datasets:\n  - name: my_dataset\n    type: snowflake    # selects SnowflakeDatasetResource\n    # ...\n\nrecipes:\n  - name: my_recipe\n    type: python       # selects PythonRecipeResource\n    # ...\n</code></pre> <p>Available dataset types: <code>snowflake</code>, <code>oracle</code>, <code>filesystem</code>, <code>upload</code>.</p> <p>Available exposed object types: <code>dataset</code>, <code>managed_folder</code>.</p> <p>Available recipe types: <code>python</code>, <code>sql_query</code>, <code>sync</code>.</p>"},{"location":"concepts/configuration/#modules","title":"Modules","text":"<p>The <code>modules</code> section lets you define reusable resource generators as Python functions. A module is a callable that accepts parameters and returns <code>list[Resource]</code>, expanded at config-load time before the engine sees them.</p> <pre><code>modules:\n  # Multiple instances of the same module \u2014 each key becomes name=\n  - call: snowflake_pipeline\n    instances:\n      customers:\n        table: CUSTOMERS\n      orders:\n        table: ORDERS\n\n  # Single invocation with explicit parameters\n  - call: modules.pipelines:customer_pipeline\n    with:\n      name: customers\n      table: CUSTOMERS\n</code></pre> <p>Module callables are resolved in three ways:</p> <ol> <li>Entry point \u2014 short name (no <code>:</code>) resolved via <code>dss_provisioner.modules</code> entry point group</li> <li>Installed package \u2014 <code>module.path:function</code> resolved via <code>importlib.import_module</code></li> <li>Local file \u2014 when the import fails, falls back to loading from a file relative to the config directory</li> </ol> <p>Module-generated resources are merged with top-level resources \u2014 the engine treats them identically. See YAML configuration for the full field reference.</p>"},{"location":"concepts/configuration/#variable-substitution","title":"Variable substitution","text":"<p>DSS variables like <code>${projectKey}</code> are supported in string fields. They are resolved transparently during plan comparison so they don't cause false drift. For example:</p> <pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"    # resolved to \"MY_PROJECT/raw\" during planning\n</code></pre>"},{"location":"concepts/configuration/#validation","title":"Validation","text":"<p>Run <code>dss-provisioner validate</code> to check your config file without connecting to DSS:</p> <pre><code>$ dss-provisioner validate\nConfiguration is valid.\n\n$ dss-provisioner validate --config custom-config.yaml\nConfiguration is valid.\n</code></pre> <p>Validation catches:</p> <ul> <li>Missing required fields (e.g., recipe <code>outputs</code>, SQL recipe <code>inputs</code>)</li> <li>Invalid type discriminators</li> <li>Pydantic type/constraint errors (name pattern <code>^[a-zA-Z0-9_]+$</code>, non-empty strings, hex color format)</li> <li>Invalid YAML syntax</li> </ul> <p>At plan time, the engine additionally validates:</p> <ul> <li><code>depends_on</code> addresses reference known resources</li> <li><code>zone</code> references point to actual zone resources</li> <li><code>sql_query</code> recipes have at least one SQL-capable input (local SQL dataset or foreign ref)</li> <li><code>exposed_objects</code> reference local DSS objects that exist</li> <li><code>foreign_*</code> source project differs from the target project</li> </ul> <p>See YAML configuration for the full field reference.</p>"},{"location":"concepts/resources/","title":"Resources","text":"<p>Resources are Pydantic models that describe the desired state of a DSS object. They are pure data \u2014 handlers know how to CRUD them.</p>"},{"location":"concepts/resources/#resource-model","title":"Resource model","text":"<p>Every resource has:</p> Field Description <code>name</code> Unique name within the DSS project. Must match <code>^[a-zA-Z0-9_]+$</code> (letters, digits, underscores) <code>description</code> Optional description (stored as DSS metadata) <code>tags</code> Optional list of tags (each element must be a non-empty string) <code>depends_on</code> Explicit dependencies on other resources. Validated at plan time \u2014 each address must exist <code>address</code> Computed as <code>{resource_type}.{name}</code> (e.g., <code>dss_filesystem_dataset.raw_data</code>) <p>The <code>address</code> is the primary key used in state tracking and plan output.</p>"},{"location":"concepts/resources/#variables-resource","title":"Variables resource","text":"<p>The <code>variables</code> resource manages DSS project variables \u2014 a singleton key-value store per project. Variables are split into two scopes:</p> <ul> <li><code>standard</code>: shared across all instances</li> <li><code>local</code>: instance-specific overrides</li> </ul> Field Type Default Description <code>name</code> <code>str</code> <code>variables</code> Resource name (singleton \u2014 rarely overridden) <code>standard</code> <code>dict[str, Any]</code> <code>{}</code> Standard project variables <code>local</code> <code>dict[str, Any]</code> <code>{}</code> Local project variables <pre><code>variables:\n  standard:\n    env: prod\n    data_root: /mnt/data\n  local:\n    debug: \"false\"\n</code></pre> <p>Variables use merge/partial semantics: only declared keys are managed. Extra keys in DSS are left alone.</p> <p>Variables have <code>plan_priority: 0</code>, so they are always applied before other resources (zones, datasets, recipes all have priority 100). This ensures variables referenced via <code>${\u2026}</code> in other resources are set before those resources are created.</p>"},{"location":"concepts/resources/#code-environment-defaults","title":"Code environment defaults","text":"<p>The <code>code_envs</code> resource selects existing instance-level code environments as the project defaults. Code environments are not created or managed by the provisioner \u2014 only the project-level default pointers are set.</p> Field Type Default Description <code>name</code> <code>str</code> <code>code_envs</code> Fixed singleton name <code>default_python</code> <code>str \\| None</code> <code>None</code> Default Python code environment name <code>default_r</code> <code>str \\| None</code> <code>None</code> Default R code environment name <pre><code>code_envs:\n  default_python: py39_ml\n  default_r: r_base\n</code></pre> <p>Both fields are optional. Omitting a field means \"don't manage this default.\" Only fields that are set are validated and applied \u2014 the provisioner calls <code>client.list_code_envs()</code> at plan time to verify referenced environments exist on the DSS instance.</p> <p>Code environment defaults have <code>plan_priority: 5</code>, so they are applied after variables (0) but before libraries (10) and other resources.</p>"},{"location":"concepts/resources/#zone-resources","title":"Zone resources","text":"<p>Zones partition a project's flow into logical sections (e.g. raw, curated, reporting). They are provisioned before datasets and recipes so that resources can reference them via the <code>zone</code> field.</p> Field Type Default Description <code>name</code> <code>str</code> \u2014 Zone identifier (must match <code>^[a-zA-Z0-9_]+$</code>) <code>color</code> <code>str</code> <code>#2ab1ac</code> Hex color in <code>#RRGGBB</code> format <pre><code>zones:\n  - name: raw\n    color: \"#4a90d9\"\n  - name: curated\n    color: \"#7b61ff\"\n</code></pre> <p>Note</p> <p>Flow zones require DSS Enterprise. On Free Edition the zone API returns 404: <code>read</code> and <code>delete</code> degrade gracefully (return None / no-op), while <code>create</code> and <code>update</code> raise a clear <code>RuntimeError</code> since the zone cannot actually be provisioned.</p>"},{"location":"concepts/resources/#git-library-resources","title":"Git library resources","text":"<p>Git libraries import external Git repositories into a project's library, making shared code available to recipes. Each library entry maps to a Git reference in DSS's <code>external-libraries.json</code>.</p> Field Type Default Description <code>name</code> <code>str</code> \u2014 Library key / single directory name in the project library (must match <code>^[a-zA-Z0-9_]+$</code>; nested paths not supported) <code>repository</code> <code>str</code> \u2014 Git repository URL (non-empty) <code>checkout</code> <code>str</code> <code>main</code> Branch, tag, or commit hash <code>path</code> <code>str</code> <code>\"\"</code> Subpath within the Git repository <code>add_to_python_path</code> <code>bool</code> <code>true</code> Add to <code>pythonPath</code> in <code>external-libraries.json</code> <pre><code>libraries:\n  - name: shared_utils\n    repository: git@github.com:org/dss-shared-lib.git\n    checkout: main\n    path: python\n</code></pre> <p>Libraries have <code>plan_priority: 10</code>, so they are applied after variables (0) but before datasets and recipes (100). This ensures library code is available before recipes that import from it are created.</p> <p>Note</p> <p><code>add_to_python_path</code> is a create-time-only field. Changing it after creation requires deleting and recreating the library. Credentials (SSH keys) are configured at the DSS instance level \u2014 no <code>login</code>/<code>password</code> fields are needed in the YAML config.</p>"},{"location":"concepts/resources/#managed-folder-resources","title":"Managed folder resources","text":"<p>Managed folders store arbitrary files (models, reports, artifacts) and are commonly used as recipe I/O. Unlike datasets, DSS accesses managed folders by an internal ID \u2014 the provisioner resolves names automatically.</p> <p>All managed folders share common fields from <code>ManagedFolderResource</code>:</p> Field Type Default Description <code>connection</code> <code>str</code> \u2014 DSS connection name <code>zone</code> <code>str</code> \u2014 Flow zone (Enterprise only) <p>Metadata (description/tags) is stored inside the managed folder settings, unlike datasets which use a separate metadata API.</p>"},{"location":"concepts/resources/#supported-types","title":"Supported types","text":"Type YAML <code>type</code> Extra required fields Filesystem <code>filesystem</code> <code>connection</code>, <code>path</code> Upload <code>upload</code> \u2014"},{"location":"concepts/resources/#filesystem-managed-folders","title":"Filesystem managed folders","text":"<pre><code>managed_folders:\n  - name: trained_models\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/models\"\n</code></pre> <p>The <code>path</code> field supports DSS variable substitution \u2014 <code>${projectKey}</code> is resolved transparently during plan comparison.</p>"},{"location":"concepts/resources/#upload-managed-folders","title":"Upload managed folders","text":"<pre><code>managed_folders:\n  - name: reports\n    type: upload\n</code></pre>"},{"location":"concepts/resources/#exposed-object-resources","title":"Exposed object resources","text":"<p>Exposed objects are project-level sharing rules: they declare which local datasets or managed folders this project makes available to other projects.</p> Field Type Default Description <code>type</code> <code>dataset \\| managed_folder</code> \u2014 Required. Exposed object type <code>target_projects</code> <code>list[str]</code> \u2014 Required. Target project keys (deduplicated) <pre><code>exposed_objects:\n  - name: curated_customers\n    type: dataset\n    target_projects:\n      - ANALYTICS\n      - REPORTING\n\n  - name: model_artifacts\n    type: managed_folder\n    target_projects:\n      - ML_SERVING\n</code></pre> <p>Exposed object resources have <code>plan_priority: 150</code>, so they run after local datasets/folders are in place.</p>"},{"location":"concepts/resources/#foreign-object-resources","title":"Foreign object resources","text":"<p>Foreign resources declare cross-project inputs this project consumes. They do not create datasets/folders in the source project; they validate that the source object exists and is exposed to this project.</p>"},{"location":"concepts/resources/#foreign-datasets","title":"Foreign datasets","text":"Field Type Default Description <code>source_project</code> <code>str</code> \u2014 Required. Source project key <code>source_name</code> <code>str</code> \u2014 Required. Source dataset name <pre><code>foreign_datasets:\n  - name: shared_customers\n    source_project: DATA_LAKE\n    source_name: curated_customers\n</code></pre>"},{"location":"concepts/resources/#foreign-managed-folders","title":"Foreign managed folders","text":"Field Type Default Description <code>source_project</code> <code>str</code> \u2014 Required. Source project key <code>source_name</code> <code>str</code> \u2014 Required. Source managed folder name <pre><code>foreign_managed_folders:\n  - name: shared_models\n    source_project: MODELING\n    source_name: model_artifacts\n</code></pre> <p>Foreign resources use the same namespace as local objects (<code>dataset</code> / <code>managed_folder</code>), so names must be unique across local + foreign declarations.</p> <p>Recipes can reference foreign aliases (<code>shared_customers</code>) in <code>inputs</code>/<code>outputs</code>; the engine resolves them to DSS full refs (<code>DATA_LAKE.curated_customers</code>) during planning/apply.</p>"},{"location":"concepts/resources/#dataset-resources","title":"Dataset resources","text":"<p>All datasets share common fields from <code>DatasetResource</code>:</p> Field Type Default Description <code>connection</code> <code>str</code> \u2014 DSS connection name <code>managed</code> <code>bool</code> <code>false</code> Whether DSS manages the data lifecycle <code>format_type</code> <code>str</code> \u2014 Storage format (e.g., <code>parquet</code>, <code>csv</code>) <code>format_params</code> <code>dict</code> <code>{}</code> Format-specific parameters <code>columns</code> <code>list[Column]</code> <code>[]</code> Schema columns <code>zone</code> <code>str</code> \u2014 Flow zone (Enterprise only)"},{"location":"concepts/resources/#supported-types_1","title":"Supported types","text":"Type YAML <code>type</code> Extra required fields Snowflake <code>snowflake</code> <code>connection</code>, <code>schema_name</code>, <code>table</code> Oracle <code>oracle</code> <code>connection</code>, <code>schema_name</code>, <code>table</code> Filesystem <code>filesystem</code> <code>connection</code>, <code>path</code> Upload <code>upload</code> \u2014"},{"location":"concepts/resources/#snowflake-datasets","title":"Snowflake datasets","text":"<pre><code>datasets:\n  - name: my_table\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n    catalog: MY_DB          # optional\n    write_mode: OVERWRITE   # OVERWRITE (default), APPEND, or TRUNCATE\n</code></pre>"},{"location":"concepts/resources/#filesystem-datasets","title":"Filesystem datasets","text":"<pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n    format_type: parquet\n    managed: true\n</code></pre> <p>The <code>path</code> field supports DSS variable substitution \u2014 <code>${projectKey}</code> is resolved transparently during plan comparison.</p>"},{"location":"concepts/resources/#upload-datasets","title":"Upload datasets","text":"<pre><code>datasets:\n  - name: lookup_table\n    type: upload\n    format_type: csv\n    format_params:\n      separator: \",\"\n      charset: utf-8\n</code></pre> <p>Upload datasets are always managed (<code>managed: true</code> by default).</p>"},{"location":"concepts/resources/#recipe-resources","title":"Recipe resources","text":"<p>All recipes share common fields from <code>RecipeResource</code>:</p> Field Type Default Description <code>inputs</code> <code>str \\| list[str]</code> <code>[]</code> Input dataset name(s). Required for SQL recipes (min 1) <code>outputs</code> <code>str \\| list[str]</code> \u2014 Required. Output dataset name(s) (min 1 element) <code>zone</code> <code>str</code> \u2014 Flow zone (Enterprise only). Validated at plan time \u2014 must reference a known zone <p>Recipe <code>inputs</code> and <code>outputs</code> create implicit dependencies \u2014 the engine automatically orders recipes after their input datasets.</p>"},{"location":"concepts/resources/#supported-types_2","title":"Supported types","text":"Type YAML <code>type</code> Extra fields Python <code>python</code> <code>code</code> or <code>code_file</code>, <code>code_env</code>, <code>code_wrapper</code> SQL Query <code>sql_query</code> <code>code</code> or <code>code_file</code> Sync <code>sync</code> \u2014"},{"location":"concepts/resources/#python-recipes","title":"Python recipes","text":"<pre><code>recipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py  # loaded at plan time\n    code_env: py311_pandas                   # optional code environment\n</code></pre> <p>You can provide code inline or via <code>code_file</code>. If both are set, <code>code_file</code> takes precedence. The <code>code_wrapper</code> flag controls whether the code runs in DSS's managed I/O wrapper.</p>"},{"location":"concepts/resources/#sql-query-recipes","title":"SQL query recipes","text":"<pre><code>recipes:\n  - name: aggregate_orders\n    type: sql_query\n    inputs: orders_raw\n    outputs: orders_summary\n    code_file: ./recipes/aggregate_orders.sql\n</code></pre> <p>SQL recipes must have at least one SQL-capable input. Inputs declared as foreign refs (<code>PROJECT.dataset</code>) or via <code>foreign_datasets</code> are accepted and validated by DSS at runtime.</p>"},{"location":"concepts/resources/#sync-recipes","title":"Sync recipes","text":"<pre><code>recipes:\n  - name: sync_customers\n    type: sync\n    inputs: customers_raw\n    outputs: customers_synced\n</code></pre>"},{"location":"concepts/resources/#scenario-resources","title":"Scenario resources","text":"<p>Scenarios define automated workflows in DSS \u2014 triggers (when to run) and actions (what to do). They are provisioned after datasets and recipes (<code>plan_priority: 200</code>) since scenario steps often reference them.</p> <p>All scenarios share common fields from <code>ScenarioResource</code>:</p> Field Type Default Description <code>active</code> <code>bool</code> <code>true</code> Whether the scenario is enabled <code>triggers</code> <code>list[dict]</code> <code>[]</code> Trigger definitions (temporal, dataset change, etc.)"},{"location":"concepts/resources/#supported-types_3","title":"Supported types","text":"Type YAML <code>type</code> Extra fields Step-based <code>step_based</code> <code>steps</code> Python <code>python</code> <code>code</code> or <code>code_file</code>"},{"location":"concepts/resources/#step-based-scenarios","title":"Step-based scenarios","text":"<pre><code>scenarios:\n  - name: daily_build\n    type: step_based\n    active: true\n    triggers:\n      - type: temporal\n        params:\n          frequency: Daily\n          hour: 2\n          minute: 0\n    steps:\n      - type: build_flowitem\n        name: Build all datasets\n        params:\n          builds:\n            - type: DATASET\n              itemId: my_dataset\n              partitionsSpec: \"\"\n</code></pre>"},{"location":"concepts/resources/#python-scenarios","title":"Python scenarios","text":"<pre><code>scenarios:\n  - name: e2e_test\n    type: python\n    active: false\n    code_file: ./scenarios/e2e_test.py\n</code></pre> <p>You can provide code inline or via <code>code_file</code>. If neither is set, the provisioner looks for <code>scenarios/{name}.py</code> by convention.</p> <p>Note</p> <p>Triggers and steps use a desired-echo strategy: the provisioner stores your declared values and echoes them on read, rather than reading back from DSS. This avoids false drift from auto-generated fields that DSS adds internally.</p>"},{"location":"concepts/resources/#columns","title":"Columns","text":"<p>Define schema columns on datasets:</p> <pre><code>datasets:\n  - name: customers\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/customers\"\n    columns:\n      - name: id\n        type: int\n        description: Customer ID\n      - name: email\n        type: string\n      - name: score\n        type: double\n        meaning: customer_score  # optional DSS meaning\n</code></pre> <p>Supported column types: <code>string</code>, <code>int</code>, <code>bigint</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>date</code>, <code>array</code>, <code>object</code>, <code>map</code>.</p>"},{"location":"guides/","title":"Guides","text":"<p>Step-by-step guides for common tasks.</p> <ul> <li>Installation \u2014 Install from source or set up for development</li> <li>End-to-end examples \u2014 Canonical Free/Enterprise, modules, and Python API examples</li> <li>YAML configuration \u2014 Full field reference for all resource types</li> <li>Writing modules \u2014 Create reusable resource generators in Python</li> <li>Python API \u2014 Use dss-provisioner programmatically as a library</li> </ul>"},{"location":"guides/examples/","title":"End-to-End Examples","text":"<p>Use these canonical examples as your starting point. They are maintained to match current resource support.</p>"},{"location":"guides/examples/#choose-an-edition","title":"Choose an edition","text":"<ul> <li>DSS Free Edition: <code>examples/free</code></li> <li>DSS Enterprise: <code>examples/enterprise</code></li> </ul>"},{"location":"guides/examples/#focused-patterns","title":"Focused patterns","text":"<ul> <li>Modules (<code>with</code> + <code>instances</code>): <code>examples/modules</code></li> <li>Python API workflow (<code>load -&gt; plan -&gt; apply</code>): <code>examples/python_api</code></li> </ul>"},{"location":"guides/examples/#coverage","title":"Coverage","text":"<p>These example sets collectively cover all supported resource categories:</p> <ul> <li>Variables and project code environment defaults</li> <li>Zones (Enterprise)</li> <li>Git libraries</li> <li>Managed folders (<code>filesystem</code>, <code>upload</code>)</li> <li>Datasets (<code>filesystem</code>, <code>upload</code>, <code>snowflake</code>, <code>oracle</code>)</li> <li>Exposed objects (<code>dataset</code>, <code>managed_folder</code>) (Enterprise)</li> <li>Foreign objects (<code>foreign_datasets</code>, <code>foreign_managed_folders</code>) (Enterprise)</li> <li>Recipes (<code>python</code>, <code>sql_query</code>, <code>sync</code>)</li> <li>Scenarios (<code>step_based</code>, <code>python</code>)</li> <li>Modules (<code>with</code>, <code>instances</code>)</li> </ul>"},{"location":"guides/examples/#suggested-workflow","title":"Suggested workflow","text":"<ol> <li>Copy one example directory into your repo.</li> <li>Replace project key, connection names, and source project references.</li> <li>Set credentials via <code>DSS_HOST</code> and <code>DSS_API_KEY</code>.</li> <li>Run <code>dss-provisioner plan</code> and review the output.</li> <li>Run <code>dss-provisioner apply</code> when the plan looks correct.</li> </ol>"},{"location":"guides/installation/","title":"Installation","text":""},{"location":"guides/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or later</li> <li>A running Dataiku DSS instance with API access</li> </ul>"},{"location":"guides/installation/#install-from-source","title":"Install from source","text":"<p>dss-provisioner is not yet published to PyPI. Install directly from GitHub:</p> <pre><code>pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre>"},{"location":"guides/installation/#with-uv","title":"With uv","text":"<pre><code>uv pip install git+https://github.com/true-north-partners/dss-provisioner.git\n</code></pre>"},{"location":"guides/installation/#development-setup","title":"Development setup","text":"<pre><code>git clone https://github.com/true-north-partners/dss-provisioner.git\ncd dss-provisioner\nuv sync\n</code></pre> <p>This installs the project in editable mode with all development dependencies.</p>"},{"location":"guides/installation/#useful-commands","title":"Useful commands","text":"<pre><code>just test       # Run tests with coverage\njust check      # Lint + format check + type check\njust format     # Auto-format code\njust build      # Build wheel and sdist\njust build_docs # Build documentation\njust serve_docs # Serve documentation locally\n</code></pre>"},{"location":"guides/installation/#verify-installation","title":"Verify installation","text":"<pre><code>$ dss-provisioner --version\ndss-provisioner 0.1.0\n\n$ dss-provisioner --help\n</code></pre>"},{"location":"guides/modules/","title":"Writing modules","text":"<p>Modules let you define reusable resource generators as Python functions. Instead of repeating similar YAML blocks, you write a function that returns <code>list[Resource]</code> and call it from your config.</p>"},{"location":"guides/modules/#your-first-module","title":"Your first module","text":"<p>Create a file next to your config:</p> <pre><code>my-project/\n\u251c\u2500\u2500 dss-provisioner.yaml\n\u2514\u2500\u2500 modules/\n    \u2514\u2500\u2500 pipelines.py\n</code></pre> modules/pipelines.py<pre><code>from dss_provisioner.resources.base import Resource\nfrom dss_provisioner.resources.dataset import FilesystemDatasetResource\n\n\ndef filesystem_pipeline(*, name: str, table: str, path_prefix: str = \"/data\") -&gt; list[Resource]:\n    \"\"\"Create a raw dataset for a given table.\"\"\"\n    return [\n        FilesystemDatasetResource(\n            name=f\"{name}_raw\",\n            connection=\"filesystem_managed\",\n            path=f\"{path_prefix}/{table.lower()}\",\n            description=f\"Raw {table} data\",\n        ),\n    ]\n</code></pre> <p>Then reference it from your config:</p> dss-provisioner.yaml<pre><code>provider:\n  project: MY_PROJECT\n\nmodules:\n  - call: modules.pipelines:filesystem_pipeline\n    instances:\n      customers:\n        table: CUSTOMERS\n      orders:\n        table: ORDERS\n        path_prefix: /staging\n</code></pre> <p>This expands to 2 datasets: <code>customers_raw</code> and <code>orders_raw</code>.</p>"},{"location":"guides/modules/#module-function-signature","title":"Module function signature","text":"<p>A module function is any Python callable that returns <code>list[Resource]</code>. It receives keyword arguments from the config.</p> <p>With <code>instances</code>, each key becomes the <code>name=</code> kwarg, and values are passed as extra kwargs:</p> <pre><code>modules:\n  - call: modules.pipelines:filesystem_pipeline\n    instances:\n      customers:          # name=\"customers\"\n        table: CUSTOMERS  # table=\"CUSTOMERS\"\n      orders:             # name=\"orders\"\n        table: ORDERS\n</code></pre> <p>With <code>with</code>, all values are passed directly as kwargs (no automatic <code>name=</code>):</p> <pre><code>modules:\n  - call: modules.pipelines:filesystem_pipeline\n    with:\n      name: customers\n      table: CUSTOMERS\n</code></pre> <p>Exactly one of <code>instances</code> or <code>with</code> must be provided.</p>"},{"location":"guides/modules/#returning-multiple-resources","title":"Returning multiple resources","text":"<p>A single function call can return any number of resources. This is where modules become powerful \u2014 one instance can generate an entire pipeline:</p> modules/pipelines.py<pre><code>from dss_provisioner.resources.base import Resource\nfrom dss_provisioner.resources.dataset import (\n    FilesystemDatasetResource,\n    UploadDatasetResource,\n)\nfrom dss_provisioner.resources.recipe import PythonRecipeResource\n\n\ndef full_pipeline(\n    *, name: str, table: str, code_file: str = \"\"\n) -&gt; list[Resource]:\n    \"\"\"Create a raw dataset, staging area, and cleaning recipe.\"\"\"\n    return [\n        FilesystemDatasetResource(\n            name=f\"{name}_raw\",\n            connection=\"filesystem_managed\",\n            path=f\"/data/{table.lower()}\",\n        ),\n        UploadDatasetResource(\n            name=f\"{name}_clean\",\n        ),\n        PythonRecipeResource(\n            name=f\"clean_{name}\",\n            inputs=f\"{name}_raw\",\n            outputs=f\"{name}_clean\",\n            code=code_file or f\"# Clean {table}\",\n        ),\n    ]\n</code></pre> <pre><code>modules:\n  - call: modules.pipelines:full_pipeline\n    instances:\n      customers:\n        table: CUSTOMERS\n        code_file: \"import dataiku\\n# ...\"\n      orders:\n        table: ORDERS\n</code></pre> <p>This generates 6 resources (3 per instance) from 8 lines of YAML.</p>"},{"location":"guides/modules/#callable-resolution","title":"Callable resolution","text":"<p>The <code>call</code> string is resolved in three ways:</p>"},{"location":"guides/modules/#1-local-file-most-common","title":"1. Local file (most common)","text":"<p>Use <code>module.path:function</code> syntax. The path is relative to your config file directory:</p> <pre><code># Loads ./modules/pipelines.py and calls filesystem_pipeline\n- call: modules.pipelines:filesystem_pipeline\n</code></pre>"},{"location":"guides/modules/#2-installed-package","title":"2. Installed package","text":"<p>If the module is installed in your Python environment (e.g., via <code>pip install</code>), the same <code>module.path:function</code> syntax resolves it via <code>importlib.import_module</code>:</p> <pre><code># Imports dss_modules_company.snowflake and calls snowflake_pipeline\n- call: dss_modules_company.snowflake:snowflake_pipeline\n</code></pre> <p>Installed packages are tried first. If not found, the resolver falls back to a local file.</p>"},{"location":"guides/modules/#3-entry-point","title":"3. Entry point","text":"<p>Use a short name (no <code>:</code>) to look up a registered entry point:</p> <pre><code># Resolved via dss_provisioner.modules entry point group\n- call: snowflake_pipeline\n</code></pre> <p>Package authors register entry points in their <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"dss_provisioner.modules\"]\nsnowflake_pipeline = \"dss_modules_company.snowflake:snowflake_pipeline\"\n</code></pre>"},{"location":"guides/modules/#error-handling","title":"Error handling","text":"<p>Module errors are caught at config load time and reported as <code>ConfigError</code>:</p> <ul> <li>Import errors \u2014 module file not found, or broken internal imports</li> <li>Missing function \u2014 module exists but the function name doesn't</li> <li>Not callable \u2014 the attribute exists but isn't a function</li> <li>Bad return type \u2014 function must return <code>list[Resource]</code></li> <li>Function exception \u2014 any exception raised by the function is wrapped with context</li> </ul>"},{"location":"guides/modules/#tips","title":"Tips","text":"<ul> <li>Keep module functions pure \u2014 they should only construct <code>Resource</code> objects, not call DSS APIs</li> <li>Use <code>**kwargs</code> to accept extra parameters gracefully if you plan to add fields later</li> <li>Module resources support all standard fields (<code>depends_on</code>, <code>tags</code>, <code>description</code>, etc.)</li> <li>Run <code>dss-provisioner validate</code> to check module expansion without connecting to DSS</li> </ul>"},{"location":"guides/python-api/","title":"Python API","text":"<p>dss-provisioner can be used as a Python library for programmatic access. All public functions are available from the <code>dss_provisioner.config</code> module.</p>"},{"location":"guides/python-api/#canonical-project-example","title":"Canonical project example","text":"<p>See <code>examples/python_api</code> for a complete layout with config, module code, recipe files, and an executable <code>app.py</code> that performs <code>load -&gt; plan -&gt; apply</code>.</p>"},{"location":"guides/python-api/#basic-usage","title":"Basic usage","text":"<pre><code>from dss_provisioner.config import load, plan, apply\n\n# Load configuration\nconfig = load(\"dss-provisioner.yaml\")\n\n# Plan changes\np = plan(config)\nprint(f\"Changes: {p.summary()}\")\n\n# Apply changes\nresult = apply(p, config)\nprint(f\"Applied: {result.summary()}\")\n</code></pre>"},{"location":"guides/python-api/#one-step-plan-and-apply","title":"One-step plan and apply","text":"<pre><code>from dss_provisioner.config import load, plan_and_apply\n\nconfig = load(\"dss-provisioner.yaml\")\nresult = plan_and_apply(config)\n</code></pre>"},{"location":"guides/python-api/#drift-detection","title":"Drift detection","text":"<pre><code>from dss_provisioner.config import load, drift\n\nconfig = load(\"dss-provisioner.yaml\")\nchanges = drift(config)\n\nfor change in changes:\n    print(f\"{change.action.value}: {change.address}\")\n    if change.diff:\n        for key, value in change.diff.items():\n            print(f\"  {key}: {value}\")\n</code></pre>"},{"location":"guides/python-api/#refresh-state","title":"Refresh state","text":"<pre><code>from dss_provisioner.config import load, refresh, save_state\n\nconfig = load(\"dss-provisioner.yaml\")\nchanges, new_state = refresh(config)\n\nif changes:\n    print(f\"Found {len(changes)} drifted resources\")\n    save_state(config, new_state)  # persist to disk\n</code></pre>"},{"location":"guides/python-api/#progress-callbacks","title":"Progress callbacks","text":"<p>Track apply progress with a callback:</p> <pre><code>from dss_provisioner.config import load, plan, apply\n\nconfig = load(\"dss-provisioner.yaml\")\np = plan(config)\n\ndef on_progress(change, event):\n    if event == \"start\":\n        print(f\"  {change.address}: applying...\")\n    else:\n        print(f\"  {change.address}: done\")\n\nresult = apply(p, config, progress=on_progress)\n</code></pre>"},{"location":"guides/python-api/#error-handling","title":"Error handling","text":"<pre><code>from dss_provisioner.config import load, plan, apply, ConfigError\nfrom dss_provisioner.engine.errors import (\n    ApplyError,\n    StalePlanError,\n    StateProjectMismatchError,\n)\n\ntry:\n    config = load(\"dss-provisioner.yaml\")\nexcept ConfigError as e:\n    print(f\"Invalid configuration: {e}\")\n    raise\n\np = plan(config)\n\ntry:\n    result = apply(p, config)\nexcept StalePlanError:\n    print(\"State changed since plan was created \u2014 re-plan required\")\nexcept ApplyError as e:\n    print(f\"Apply failed at {e.address}\")\n    print(f\"Successfully applied: {e.result.summary()}\")\n</code></pre>"},{"location":"guides/python-api/#api-reference","title":"API reference","text":"<p>See the full API reference for all classes, functions, and types.</p>"},{"location":"guides/yaml-config/","title":"YAML configuration","text":"<p>Complete reference for the <code>dss-provisioner.yaml</code> configuration file.</p> <p>For full copy/paste starter projects, see End-to-end examples.</p>"},{"location":"guides/yaml-config/#minimal-example","title":"Minimal example","text":"<pre><code>provider:\n  project: MY_PROJECT\n\ndatasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n</code></pre>"},{"location":"guides/yaml-config/#full-example","title":"Full example","text":"<pre><code>provider:\n  host: https://dss.company.com\n  # api_key: omit from YAML \u2014 set DSS_API_KEY env var instead\n  project: ANALYTICS\n\nstate_path: .dss-state.json\n\nvariables:\n  standard:\n    env: prod\n    data_root: /mnt/data\n  local:\n    debug: \"false\"\n\ncode_envs:\n  default_python: py311_pandas\n  default_r: r_base\n\nzones:\n  - name: raw\n    color: \"#4a90d9\"\n  - name: curated\n    color: \"#7b61ff\"\n\nlibraries:\n  - name: shared_utils\n    repository: git@github.com:org/dss-shared-lib.git\n    checkout: main\n    path: python\n\nmanaged_folders:\n  - name: trained_models\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/models\"\n\n  - name: reports\n    type: upload\n\ndatasets:\n  - name: customers_raw\n    type: snowflake\n    connection: snowflake_prod\n    schema_name: RAW\n    table: CUSTOMERS\n    description: Raw customer data from Snowflake\n\n  - name: customers_clean\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/clean/customers\"\n    managed: true\n    format_type: parquet\n    columns:\n      - name: id\n        type: int\n      - name: name\n        type: string\n      - name: email\n        type: string\n    tags:\n      - production\n      - pii\n\nexposed_objects:\n  - name: customers_clean\n    type: dataset\n    target_projects:\n      - ANALYTICS_APP\n      - REPORTING\n\n  - name: trained_models\n    type: managed_folder\n    target_projects:\n      - ML_SERVING\n\nforeign_datasets:\n  - name: shared_orders\n    source_project: DATA_LAKE\n    source_name: curated_orders\n\nforeign_managed_folders:\n  - name: shared_reference_data\n    source_project: GOVERNANCE\n    source_name: master_reference\n\nrecipes:\n  - name: clean_customers\n    type: python\n    inputs: customers_raw\n    outputs: customers_clean\n    code_file: ./recipes/clean_customers.py\n\n  - name: sync_customers\n    type: sync\n    inputs: customers_clean\n    outputs: customers_synced\n    depends_on:\n      - dss_python_recipe.clean_customers\n\nscenarios:\n  - name: daily_build\n    type: step_based\n    active: true\n    triggers:\n      - type: temporal\n        params:\n          frequency: Daily\n          hour: 2\n          minute: 0\n    steps:\n      - type: build_flowitem\n        name: Build all datasets\n        params:\n          builds:\n            - type: DATASET\n              itemId: customers_clean\n              partitionsSpec: \"\"\n\n  - name: e2e_test\n    type: python\n    active: false\n    code_file: ./scenarios/e2e_test.py\n\nmodules:\n  - call: modules.pipelines:snowflake_pipeline\n    instances:\n      customers:\n        table: CUSTOMERS\n        schema_name: RAW\n      orders:\n        table: ORDERS\n        schema_name: STAGING\n</code></pre>"},{"location":"guides/yaml-config/#provider","title":"Provider","text":"Field Env var Required Default Description <code>host</code> <code>DSS_HOST</code> Yes \u2014 DSS instance URL <code>api_key</code> <code>DSS_API_KEY</code> Yes \u2014 API key <code>project</code> <code>DSS_PROJECT</code> Yes \u2014 Target project key <code>verify_ssl</code> <code>DSS_VERIFY_SSL</code> No <code>true</code> Verify SSL certificates. Set <code>false</code> for self-signed certs <p>A <code>.env</code> file next to the config file is loaded automatically (priority: YAML &gt; env var &gt; <code>.env</code> &gt; default).</p>"},{"location":"guides/yaml-config/#top-level-fields","title":"Top-level fields","text":"Field Type Default Description <code>provider</code> object \u2014 DSS connection settings (required) <code>state_path</code> string <code>.dss-state.json</code> Path to state file <code>variables</code> object \u2014 Project variables (singleton, applied first) <code>code_envs</code> object \u2014 Project default code environments (applied after variables, before libraries) <code>zones</code> list <code>[]</code> Flow zone definitions (provisioned before datasets/recipes) <code>libraries</code> list <code>[]</code> Git library references (applied after variables, before datasets/recipes) <code>managed_folders</code> list <code>[]</code> Managed folder resource definitions <code>datasets</code> list <code>[]</code> Dataset resource definitions <code>exposed_objects</code> list <code>[]</code> Cross-project exposure rules for local datasets/folders <code>foreign_datasets</code> list <code>[]</code> Foreign dataset aliases from other DSS projects <code>foreign_managed_folders</code> list <code>[]</code> Foreign managed folder aliases from other DSS projects <code>recipes</code> list <code>[]</code> Recipe resource definitions <code>scenarios</code> list <code>[]</code> Scenario resource definitions (applied after datasets/recipes) <code>modules</code> list <code>[]</code> Python module invocations that expand into resources at config-load time"},{"location":"guides/yaml-config/#variables-fields","title":"Variables fields","text":"Field Type Default Description <code>name</code> string <code>variables</code> Resource name (singleton \u2014 rarely overridden). Must match <code>^[a-zA-Z0-9_]+$</code> <code>standard</code> dict <code>{}</code> Standard project variables (shared across instances) <code>local</code> dict <code>{}</code> Local project variables (instance-specific) <code>description</code> string <code>\"\"</code> Not used by DSS variables (ignored) <code>tags</code> list <code>[]</code> Not used by DSS variables (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Variables use partial semantics: only declared keys are managed. Extra keys already in DSS are preserved.</p> <p>Variables are always applied before other resource types due to their <code>plan_priority: 0</code> (other resources default to 100).</p>"},{"location":"guides/yaml-config/#code-environment-fields","title":"Code environment fields","text":"Field Type Default Description <code>name</code> string <code>code_envs</code> Fixed singleton name (cannot be changed) <code>default_python</code> string \u2014 Default Python code environment name (must exist on DSS instance) <code>default_r</code> string \u2014 Default R code environment name (must exist on DSS instance) <code>description</code> string <code>\"\"</code> Not used by DSS code envs (ignored) <code>tags</code> list <code>[]</code> Not used by DSS code envs (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Code environments are instance-scoped in DSS. The provisioner does not create or manage them \u2014 it only selects existing environments as the project default. At plan time, the engine validates that referenced environments exist by calling <code>list_code_envs()</code>.</p> <p>Code environment defaults have <code>plan_priority: 5</code>, applied after variables (0) but before libraries (10).</p>"},{"location":"guides/yaml-config/#zone-fields","title":"Zone fields","text":"Field Type Default Description <code>name</code> string \u2014 Required. Zone identifier (must match <code>^[a-zA-Z0-9_]+$</code>) <code>color</code> string <code>#2ab1ac</code> Hex color in <code>#RRGGBB</code> format <code>description</code> string <code>\"\"</code> Not used by DSS zones (ignored) <code>tags</code> list <code>[]</code> Not used by DSS zones (ignored) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p>Flow zones require DSS Enterprise. On Free Edition the zone API is unavailable.</p>"},{"location":"guides/yaml-config/#library-fields","title":"Library fields","text":"Field Type Default Description <code>name</code> string \u2014 Required. Library key (single segment in the library hierarchy; letters, digits, and underscores only). Must match <code>^[a-zA-Z0-9_]+$</code> <code>repository</code> string \u2014 Required. Git repository URL (non-empty) <code>checkout</code> string <code>main</code> Branch, tag, or commit hash to check out <code>path</code> string <code>\"\"</code> Subpath within the Git repository <code>add_to_python_path</code> bool <code>true</code> Add to <code>pythonPath</code> in <code>external-libraries.json</code> <code>description</code> string <code>\"\"</code> Not used by DSS libraries (ignored) <code>tags</code> list <code>[]</code> Not used by DSS libraries (ignored). Elements must be non-empty strings <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p><code>add_to_python_path</code> is a create-time-only field. To change it, delete and recreate the library.</p> <p>Tip</p> <p>For <code>dss-provisioner preview</code>, set <code>repository: self</code> to reuse the current repo's <code>origin</code> URL and automatically pin <code>checkout</code> to the current branch in the preview project.</p>"},{"location":"guides/yaml-config/#managed-folder-fields","title":"Managed folder fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Managed folder name in DSS. Must match <code>^[a-zA-Z0-9_]+$</code> <code>type</code> string \u2014 Required. One of: <code>filesystem</code>, <code>upload</code> <code>connection</code> string \u2014 DSS connection name <code>zone</code> string \u2014 Flow zone (Enterprise only). Validated at plan time \u2014 must reference a known zone <code>description</code> string <code>\"\"</code> Managed folder description <code>tags</code> list <code>[]</code> DSS tags. Elements must be non-empty strings <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses)"},{"location":"guides/yaml-config/#filesystem-specific-fields","title":"Filesystem-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Filesystem connection name <code>path</code> string \u2014 Required. File path (non-empty, supports <code>${projectKey}</code>)"},{"location":"guides/yaml-config/#upload-specific-fields","title":"Upload-specific fields","text":"<p>Upload managed folders have no additional required fields.</p>"},{"location":"guides/yaml-config/#dataset-fields","title":"Dataset fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types_1","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Dataset name in DSS. Must match <code>^[a-zA-Z0-9_]+$</code> <code>type</code> string \u2014 Required. One of: <code>snowflake</code>, <code>oracle</code>, <code>filesystem</code>, <code>upload</code> <code>connection</code> string \u2014 DSS connection name <code>managed</code> bool <code>false</code> Whether DSS manages the data lifecycle <code>format_type</code> string \u2014 Storage format (<code>parquet</code>, <code>csv</code>, etc.) <code>format_params</code> dict <code>{}</code> Format-specific parameters <code>columns</code> list <code>[]</code> Schema column definitions <code>zone</code> string \u2014 Flow zone (Enterprise only). Validated at plan time \u2014 must reference a known zone <code>description</code> string <code>\"\"</code> Dataset description (metadata) <code>tags</code> list <code>[]</code> DSS tags. Elements must be non-empty strings <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses). Validated at plan time \u2014 each address must exist"},{"location":"guides/yaml-config/#snowflake-specific-fields","title":"Snowflake-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Snowflake connection name <code>schema_name</code> string \u2014 Required. Snowflake schema (non-empty) <code>table</code> string \u2014 Required. Table name (non-empty) <code>catalog</code> string \u2014 Snowflake database/catalog <code>write_mode</code> string <code>OVERWRITE</code> <code>OVERWRITE</code>, <code>APPEND</code>, or <code>TRUNCATE</code>"},{"location":"guides/yaml-config/#oracle-specific-fields","title":"Oracle-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Oracle connection name <code>schema_name</code> string \u2014 Required. Oracle schema (non-empty) <code>table</code> string \u2014 Required. Table name (non-empty)"},{"location":"guides/yaml-config/#filesystem-specific-fields_1","title":"Filesystem-specific fields","text":"Field Type Default Description <code>connection</code> string \u2014 Required. Filesystem connection name <code>path</code> string \u2014 Required. File path (non-empty, supports <code>${projectKey}</code>)"},{"location":"guides/yaml-config/#upload-specific-fields_1","title":"Upload-specific fields","text":"<p>Upload datasets have no additional required fields. They default to <code>managed: true</code>.</p>"},{"location":"guides/yaml-config/#exposed-object-fields","title":"Exposed object fields","text":"<p>Use <code>exposed_objects</code> to share local datasets/folders with other projects.</p>"},{"location":"guides/yaml-config/#common-fields","title":"Common fields","text":"Field Type Default Description <code>name</code> string \u2014 Required. Local object name to expose <code>type</code> string \u2014 Required. One of: <code>dataset</code>, <code>managed_folder</code> <code>target_projects</code> list \u2014 Required. Target project keys (min 1; duplicates removed) <code>description</code> string <code>\"\"</code> Not used by DSS exposed objects (kept in state) <code>tags</code> list <code>[]</code> Not used by DSS exposed objects (kept in state) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses)"},{"location":"guides/yaml-config/#foreign-object-fields","title":"Foreign object fields","text":"<p>Use foreign resources to declare cross-project aliases consumed by this project.</p>"},{"location":"guides/yaml-config/#foreign-datasets-foreign_datasets","title":"Foreign datasets (<code>foreign_datasets</code>)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Local alias used in recipes <code>source_project</code> string \u2014 Required. Source project key <code>source_name</code> string \u2014 Required. Source dataset name <code>description</code> string <code>\"\"</code> Not used by DSS foreign refs (kept in state) <code>tags</code> list <code>[]</code> Not used by DSS foreign refs (kept in state) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses)"},{"location":"guides/yaml-config/#foreign-managed-folders-foreign_managed_folders","title":"Foreign managed folders (<code>foreign_managed_folders</code>)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Local alias used in recipes <code>source_project</code> string \u2014 Required. Source project key <code>source_name</code> string \u2014 Required. Source managed folder name <code>description</code> string <code>\"\"</code> Not used by DSS foreign refs (kept in state) <code>tags</code> list <code>[]</code> Not used by DSS foreign refs (kept in state) <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses)"},{"location":"guides/yaml-config/#recipe-fields","title":"Recipe fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types_2","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Recipe name in DSS. Must match <code>^[a-zA-Z0-9_]+$</code> <code>type</code> string \u2014 Required. One of: <code>python</code>, <code>sql_query</code>, <code>sync</code> <code>inputs</code> string or list <code>[]</code> Input refs. Can be local names, foreign aliases, or <code>PROJECT.object</code> refs. Required for <code>sql_query</code> (min 1) <code>outputs</code> string or list \u2014 Required. Output refs (min 1 element). Elements must be non-empty <code>zone</code> string \u2014 Flow zone (Enterprise only). Validated at plan time \u2014 must reference a known zone <code>description</code> string <code>\"\"</code> Recipe description <code>tags</code> list <code>[]</code> DSS tags. Elements must be non-empty strings <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses). Validated at plan time \u2014 each address must exist <p>Note</p> <p><code>inputs</code> and <code>outputs</code> accept either a single string or a list of strings. A single string is automatically converted to a one-element list.</p>"},{"location":"guides/yaml-config/#python-specific-fields","title":"Python-specific fields","text":"Field Type Default Description <code>code</code> string <code>\"\"</code> Inline Python code <code>code_file</code> string \u2014 Path to Python file (relative to config file) <code>code_env</code> string \u2014 DSS code environment name <code>code_wrapper</code> bool <code>false</code> Use DSS managed I/O wrapper"},{"location":"guides/yaml-config/#sql-query-specific-fields","title":"SQL query-specific fields","text":"Field Type Default Description <code>inputs</code> string or list \u2014 Required. Input refs (min 1 element; validated at plan time \u2014 at least one input must be SQL-capable or a foreign ref) <code>code</code> string <code>\"\"</code> Inline SQL code <code>code_file</code> string \u2014 Path to SQL file (relative to config file)"},{"location":"guides/yaml-config/#sync-specific-fields","title":"Sync-specific fields","text":"<p>Sync recipes have no additional fields beyond the common recipe fields.</p>"},{"location":"guides/yaml-config/#scenario-fields","title":"Scenario fields","text":""},{"location":"guides/yaml-config/#common-fields-all-types_3","title":"Common fields (all types)","text":"Field Type Default Description <code>name</code> string \u2014 Required. Scenario name in DSS. Must match <code>^[a-zA-Z0-9_]+$</code> <code>type</code> string \u2014 Required. One of: <code>step_based</code>, <code>python</code> <code>active</code> bool <code>true</code> Whether the scenario is enabled <code>triggers</code> list <code>[]</code> Trigger definitions (temporal, dataset change, etc.) <code>description</code> string <code>\"\"</code> Scenario description <code>tags</code> list <code>[]</code> DSS tags <code>depends_on</code> list <code>[]</code> Explicit resource dependencies (addresses) <p>Note</p> <p>Triggers and steps are stored as raw dicts matching the DSS API format. The provisioner echoes your declared values on read to avoid false drift from auto-generated fields.</p>"},{"location":"guides/yaml-config/#step-based-specific-fields","title":"Step-based-specific fields","text":"Field Type Default Description <code>steps</code> list <code>[]</code> Step definitions (build, run scenario, etc.)"},{"location":"guides/yaml-config/#python-specific-fields_1","title":"Python-specific fields","text":"Field Type Default Description <code>code</code> string <code>\"\"</code> Inline Python code <code>code_file</code> string \u2014 Path to Python file (relative to config file)"},{"location":"guides/yaml-config/#modules","title":"Modules","text":"<p>Modules let you define reusable resource generators as Python functions. Each module entry specifies a callable and how to invoke it.</p>"},{"location":"guides/yaml-config/#module-entry-fields","title":"Module entry fields","text":"Field Type Default Description <code>call</code> string \u2014 Required. Callable reference \u2014 short name (entry point) or <code>module.path:function</code> <code>instances</code> dict \u2014 Named instances. Each key becomes <code>name=</code>, values are extra kwargs <code>with</code> dict \u2014 Single invocation kwargs (passed directly to the callable) <p>Exactly one of <code>instances</code> or <code>with</code> must be provided.</p>"},{"location":"guides/yaml-config/#callable-resolution","title":"Callable resolution","text":"<p>The <code>call</code> string is resolved in order:</p> <ol> <li>Entry point \u2014 if no <code>:</code> is present, looks up the name in the <code>dss_provisioner.modules</code> entry point group</li> <li>Installed package \u2014 <code>module.path:function</code> tries <code>importlib.import_module</code> first</li> <li>Local file \u2014 falls back to loading <code>module/path.py</code> relative to the config file directory</li> </ol>"},{"location":"guides/yaml-config/#entry-point-registration","title":"Entry point registration","text":"<p>Package authors register module callables as entry points:</p> <pre><code># pyproject.toml\n[project.entry-points.\"dss_provisioner.modules\"]\nsnowflake_pipeline = \"my_package.snowflake:snowflake_pipeline\"\n</code></pre>"},{"location":"guides/yaml-config/#examples","title":"Examples","text":"<pre><code># Multiple instances \u2014 each key becomes name= kwarg\nmodules:\n  - call: snowflake_pipeline\n    instances:\n      customers:\n        table: CUSTOMERS\n      orders:\n        table: ORDERS\n\n  # Single invocation \u2014 kwargs passed directly\n  - call: modules.pipelines:customer_pipeline\n    with:\n      name: customers\n      table: CUSTOMERS\n</code></pre> <p>The callable must return <code>list[Resource]</code>. Module-generated resources are merged with top-level resources before planning.</p>"},{"location":"guides/yaml-config/#column-definition","title":"Column definition","text":"Field Type Default Description <code>name</code> string \u2014 Required. Column name (non-empty) <code>type</code> string \u2014 Required. One of: <code>string</code>, <code>int</code>, <code>bigint</code>, <code>float</code>, <code>double</code>, <code>boolean</code>, <code>date</code>, <code>array</code>, <code>object</code>, <code>map</code> <code>description</code> string <code>\"\"</code> Column description <code>meaning</code> string \u2014 DSS column meaning"},{"location":"guides/yaml-config/#validation","title":"Validation","text":"<p>All resource names must match <code>^[a-zA-Z0-9_]+$</code> (letters, digits, and underscores only). This is enforced at config load time.</p> <p>Additional parse-time constraints:</p> <ul> <li>Tags: each tag must be a non-empty string</li> <li>Recipe outputs: at least one output is required</li> <li>SQL recipe inputs: at least one input is required</li> <li>Zone color: must be a valid hex color in <code>#RRGGBB</code> format</li> <li>Snowflake/Oracle <code>schema_name</code> and <code>table</code>: must be non-empty</li> <li>Filesystem <code>path</code>: must be non-empty</li> <li>Git library <code>repository</code>: must be non-empty</li> </ul> <p>At plan time, the engine additionally validates:</p> <ul> <li><code>depends_on</code> addresses must reference a known resource (in config or state)</li> <li><code>zone</code> references must point to a resource of type <code>dss_zone</code></li> <li>SQL recipe inputs must include at least one SQL-capable input (local SQL dataset or foreign ref)</li> <li><code>exposed_objects</code> names must exist as local objects in DSS</li> <li><code>foreign_*</code> <code>source_project</code> must differ from the target project</li> <li><code>code_envs</code> <code>default_python</code> and <code>default_r</code> must reference existing code environments on the DSS instance</li> <li>Python recipe <code>code_env</code> must reference an existing Python code environment on the DSS instance</li> </ul>"},{"location":"guides/yaml-config/#dependencies","title":"Dependencies","text":"<p>Resources can depend on each other in two ways:</p>"},{"location":"guides/yaml-config/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Use <code>depends_on</code> with full resource addresses:</p> <pre><code>recipes:\n  - name: aggregate\n    type: python\n    depends_on:\n      - dss_python_recipe.clean_data\n</code></pre>"},{"location":"guides/yaml-config/#implicit-dependencies","title":"Implicit dependencies","text":"<p>Recipe <code>inputs</code> and <code>outputs</code> automatically create dependencies on referenced local/foreign resources by name. You don't need to add <code>depends_on</code> for these in the common case.</p> <pre><code>datasets:\n  - name: raw_data\n    type: filesystem\n    connection: filesystem_managed\n    path: \"${projectKey}/raw\"\n\nrecipes:\n  - name: process\n    type: python\n    inputs: raw_data      # automatically depends on dss_filesystem_dataset.raw_data\n    outputs: clean_data\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>Detailed reference documentation.</p> <ul> <li>CLI \u2014 All commands, options, and usage examples</li> <li>Resources \u2014 Auto-generated docs for resource classes</li> <li>API \u2014 Auto-generated docs for the full Python API</li> </ul>"},{"location":"reference/api/","title":"API reference","text":"<p>Auto-generated documentation from source code docstrings.</p>"},{"location":"reference/api/#public-api","title":"Public API","text":"<p>The main entry point for programmatic use. All functions are importable from <code>dss_provisioner.config</code>.</p>"},{"location":"reference/api/#dss_provisioner.config","title":"<code>dss_provisioner.config</code>","text":"<p>YAML configuration loading and convenience plan/apply API.</p>"},{"location":"reference/api/#dss_provisioner.config.load","title":"<code>load(path)</code>","text":"<p>Load a YAML configuration file.</p>"},{"location":"reference/api/#dss_provisioner.config.plan","title":"<code>plan(config, *, destroy=False, refresh=True)</code>","text":"<p>Plan changes for the given configuration.</p>"},{"location":"reference/api/#dss_provisioner.config.apply","title":"<code>apply(plan_obj, config, *, progress=None)</code>","text":"<p>Apply a previously computed plan.</p>"},{"location":"reference/api/#dss_provisioner.config.plan_and_apply","title":"<code>plan_and_apply(config, *, destroy=False, refresh=True)</code>","text":"<p>Plan and apply in one step.</p>"},{"location":"reference/api/#dss_provisioner.config.refresh","title":"<code>refresh(config)</code>","text":"<p>Refresh state from the live DSS instance (not persisted).</p> <p>Returns the list of drift changes and the new state. Call :func:<code>save_state</code> to persist the returned state to disk.</p>"},{"location":"reference/api/#dss_provisioner.config.save_state","title":"<code>save_state(config, state)</code>","text":"<p>Persist state to disk.</p>"},{"location":"reference/api/#dss_provisioner.config.drift","title":"<code>drift(config)</code>","text":"<p>Detect drift between state file and live DSS.</p>"},{"location":"reference/api/#configuration","title":"Configuration","text":""},{"location":"reference/api/#config","title":"Config","text":""},{"location":"reference/api/#dss_provisioner.config.schema.Config","title":"<code>dss_provisioner.config.schema.Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Provisioning configuration \u2014 validates YAML structure directly.</p>"},{"location":"reference/api/#dss_provisioner.config.schema.Config.resources","title":"<code>resources</code>  <code>property</code>","text":"<p>All declared resources \u2014 ordering is not significant.</p>"},{"location":"reference/api/#providerconfig","title":"ProviderConfig","text":""},{"location":"reference/api/#dss_provisioner.config.schema.ProviderConfig","title":"<code>dss_provisioner.config.schema.ProviderConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DSS provider connection settings.</p> <p>Fields can be set via YAML, environment variables (<code>DSS_HOST</code>, <code>DSS_API_KEY</code>, <code>DSS_PROJECT</code>, <code>DSS_VERIFY_SSL</code>), or a <code>.env</code> file next to the config file.</p> <p>Priority (highest wins): YAML value &gt; env var &gt; <code>.env</code> file &gt; default.</p> <p>Resolution is handled by :func:<code>~dss_provisioner.config.loader._resolve_provider</code>.</p>"},{"location":"reference/api/#engine-types","title":"Engine types","text":""},{"location":"reference/api/#plan","title":"Plan","text":""},{"location":"reference/api/#dss_provisioner.engine.types.Plan","title":"<code>dss_provisioner.engine.types.Plan</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#resourcechange","title":"ResourceChange","text":""},{"location":"reference/api/#dss_provisioner.engine.types.ResourceChange","title":"<code>dss_provisioner.engine.types.ResourceChange</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#action","title":"Action","text":""},{"location":"reference/api/#dss_provisioner.engine.types.Action","title":"<code>dss_provisioner.engine.types.Action</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"reference/api/#applyresult","title":"ApplyResult","text":""},{"location":"reference/api/#dss_provisioner.engine.types.ApplyResult","title":"<code>dss_provisioner.engine.types.ApplyResult</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#planmetadata","title":"PlanMetadata","text":""},{"location":"reference/api/#dss_provisioner.engine.types.PlanMetadata","title":"<code>dss_provisioner.engine.types.PlanMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/api/#engine","title":"Engine","text":""},{"location":"reference/api/#dssengine","title":"DSSEngine","text":""},{"location":"reference/api/#dss_provisioner.engine.engine.DSSEngine","title":"<code>dss_provisioner.engine.engine.DSSEngine(*, provider, project_key, state_path, registry)</code>","text":"<p>Terraform-like plan/apply engine for DSS resources.</p>"},{"location":"reference/api/#dss_provisioner.engine.engine.DSSEngine.refresh","title":"<code>refresh(*, persist=False)</code>","text":"<p>Refresh state from DSS. Returns (pre_refresh, post_refresh).</p>"},{"location":"reference/api/#state","title":"State","text":""},{"location":"reference/api/#state_1","title":"State","text":""},{"location":"reference/api/#dss_provisioner.core.state.State","title":"<code>dss_provisioner.core.state.State</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Terraform-style state file for tracking deployed resources.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>State file format version</p> <code>project_key</code> <code>str</code> <p>DSS project key</p> <code>resources</code> <code>dict[str, ResourceInstance]</code> <p>Mapping of resource addresses to instances</p> <code>outputs</code> <code>dict[str, Any]</code> <p>Output values from the configuration</p>"},{"location":"reference/api/#dss_provisioner.core.state.State.save","title":"<code>save(path)</code>","text":"<p>Save state to a JSON file.</p> <ul> <li>Writes atomically (temp file + rename)</li> <li>Writes a <code>.backup</code> copy of the previous state when overwriting</li> </ul>"},{"location":"reference/api/#dss_provisioner.core.state.State.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load state from a JSON file.</p>"},{"location":"reference/api/#dss_provisioner.core.state.State.load_or_create","title":"<code>load_or_create(path, project_key)</code>  <code>classmethod</code>","text":"<p>Load existing state or create a new one.</p>"},{"location":"reference/api/#resourceinstance","title":"ResourceInstance","text":""},{"location":"reference/api/#dss_provisioner.core.state.ResourceInstance","title":"<code>dss_provisioner.core.state.ResourceInstance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A tracked resource instance in the state file.</p> <p>Attributes:</p> Name Type Description <code>address</code> <code>str</code> <p>Unique resource address (e.g., \"dss_recipe.join_orders\")</p> <code>resource_type</code> <code>str</code> <p>Type of the resource (e.g., \"dss_join_recipe\")</p> <code>name</code> <code>str</code> <p>Resource name (e.g., \"join_orders\")</p> <code>attributes</code> <code>dict[str, Any]</code> <p>Current attribute values</p> <code>attributes_hash</code> <code>str</code> <p>SHA256 hash for change detection</p> <code>dependencies</code> <code>list[str]</code> <p>Addresses of dependencies</p> <code>created_at</code> <code>datetime</code> <p>When the resource was created</p> <code>updated_at</code> <code>datetime</code> <p>When the resource was last updated</p>"},{"location":"reference/api/#provider","title":"Provider","text":""},{"location":"reference/api/#dssprovider","title":"DSSProvider","text":""},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider","title":"<code>dss_provisioner.core.provider.DSSProvider</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Connection configuration for a DSS instance.</p> <p>For external use, provide host and auth. For internal use (inside DSS notebooks/recipes), use the <code>from_client</code> classmethod to inject a client.</p> <p>Examples:</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider--external-with-api-key","title":"External with API key","text":"<p>provider = DSSProvider(     host=\"https://dss.company.com\",     auth=ApiKeyAuth(api_key=\"my-api-key\"), )</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider--inside-dss-notebook","title":"Inside DSS notebook","text":"<p>import dataiku provider = DSSProvider.from_client(dataiku.api_client())</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.client","title":"<code>client</code>  <code>cached</code> <code>property</code>","text":"<p>Get the DSS client.</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.from_client","title":"<code>from_client(client)</code>  <code>classmethod</code>","text":"<p>Create a provider with an injected client.</p> <p>Use this for running inside DSS or for testing with a mock client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>DSSClient</code> <p>A pre-configured DSSClient instance</p> required Example <p>import dataiku provider = DSSProvider.from_client(dataiku.api_client())</p>"},{"location":"reference/api/#dss_provisioner.core.provider.DSSProvider.in_project","title":"<code>in_project(project_key)</code>","text":"<p>Bind this provider to a single project for convenience.</p>"},{"location":"reference/api/#apikeyauth","title":"ApiKeyAuth","text":""},{"location":"reference/api/#dss_provisioner.core.provider.ApiKeyAuth","title":"<code>dss_provisioner.core.provider.ApiKeyAuth</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>API key authentication for DSS.</p>"},{"location":"reference/api/#errors","title":"Errors","text":""},{"location":"reference/api/#dss_provisioner.engine.errors","title":"<code>dss_provisioner.engine.errors</code>","text":"<p>Engine error types.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for engine errors.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.UnknownResourceTypeError","title":"<code>UnknownResourceTypeError(resource_type)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when a resource type has no registration/handler.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.DuplicateAddressError","title":"<code>DuplicateAddressError(address)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when multiple desired resources share the same address.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.DependencyCycleError","title":"<code>DependencyCycleError(addresses)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when dependencies contain a cycle.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StateProjectMismatchError","title":"<code>StateProjectMismatchError(expected, got)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when the on-disk state belongs to a different project.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StalePlanError","title":"<code>StalePlanError</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when applying a plan against a different state than planned.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.StateLockError","title":"<code>StateLockError</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when the state lock cannot be acquired or released.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ValidationError","title":"<code>ValidationError(errors)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>One or more resources failed plan validation.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ApplyError","title":"<code>ApplyError(*, applied, address, message)</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when an apply fails mid-way through.</p> <p>Carries the partial result (what was applied before the failure) so callers can inspect progress.  The original exception is chained via <code>__cause__</code>.</p>"},{"location":"reference/api/#dss_provisioner.engine.errors.ApplyCanceled","title":"<code>ApplyCanceled</code>","text":"<p>               Bases: <code>EngineError</code></p> <p>Raised when an apply is canceled (e.g., Ctrl-C).</p>"},{"location":"reference/cli/","title":"CLI reference","text":"<p>dss-provisioner provides a Typer-based CLI with seven commands.</p>"},{"location":"reference/cli/#global-options","title":"Global options","text":"<p>All commands accept:</p> Option Default Description <code>--config</code> <code>dss-provisioner.yaml</code> Path to configuration file <code>--no-color</code> <code>false</code> Disable colored output <code>-v</code> / <code>-vv</code> \u2014 Increase log verbosity (<code>-v</code> info, <code>-vv</code> debug) <p>Verbosity flags are top-level options and must appear before the command name:</p> <pre><code>dss-provisioner -v plan          # INFO-level logs on stderr\ndss-provisioner -vv apply        # DEBUG-level logs on stderr\n</code></pre> <p>The <code>DSS_LOG</code> environment variable overrides <code>-v</code> flags and accepts any Python logging level name (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>):</p> <pre><code>DSS_LOG=DEBUG dss-provisioner plan   # same as -vv\n</code></pre> <p>Logs are scoped to <code>dss_provisioner.*</code> loggers and written to stderr so they do not interfere with command output on stdout.</p>"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#plan","title":"<code>plan</code>","text":"<p>Show changes required by the current configuration.</p> <pre><code>dss-provisioner plan [OPTIONS]\n</code></pre> Option Default Description <code>--out PATH</code> \u2014 Save plan to a JSON file for later <code>apply</code> <code>--no-refresh</code> <code>false</code> Skip refreshing state from DSS before planning <p>By default, <code>plan</code> refreshes state from the live DSS instance before computing the diff. Use <code>--no-refresh</code> to plan against the local state file only.</p>"},{"location":"reference/cli/#apply","title":"<code>apply</code>","text":"<p>Apply the changes required by the current configuration.</p> <pre><code>dss-provisioner apply [PLAN_FILE] [OPTIONS]\n</code></pre> Option Default Description <code>PLAN_FILE</code> \u2014 Path to a saved plan (from <code>plan --out</code>) <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <code>--no-refresh</code> <code>false</code> Skip refreshing state before planning <p>If <code>PLAN_FILE</code> is provided, apply uses the saved plan (checking for staleness). Otherwise, it runs <code>plan</code> + <code>apply</code> in one step.</p>"},{"location":"reference/cli/#destroy","title":"<code>destroy</code>","text":"<p>Destroy all managed resources.</p> <pre><code>dss-provisioner destroy [OPTIONS]\n</code></pre> Option Default Description <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <p>Plans deletion of all resources tracked in the state file, then applies in reverse dependency order.</p>"},{"location":"reference/cli/#preview","title":"<code>preview</code>","text":"<p>Create, list, or destroy branch-based preview projects.</p> <pre><code>dss-provisioner preview [OPTIONS]\n</code></pre> Option Default Description <code>--branch TEXT</code> current git branch Override branch used for preview key and <code>repository: self</code> library checkout <code>--destroy</code> <code>false</code> Delete the computed preview project and preview state files <code>--list</code> <code>false</code> List active preview projects for the configured base project <code>--force</code> <code>false</code> Allow reuse/delete of an existing project key even when it is not tagged as a provisioner-managed preview <code>--no-refresh</code> <code>false</code> Skip refreshing preview state before planning <p><code>preview</code> derives a project key from <code>provider.project</code> + branch (for example <code>ANALYTICS__FEATURE_X</code>), rewrites <code>repository: self</code> libraries to the local <code>origin</code> URL at that branch, applies the config to the preview project, and uses an isolated state file (for example <code>.dss-state.preview.feature_x.json</code>). By default, reuse/delete operations only proceed for projects tagged as provisioner-managed previews; <code>--force</code> overrides that guard.</p>"},{"location":"reference/cli/#refresh","title":"<code>refresh</code>","text":"<p>Refresh state from the live DSS instance.</p> <pre><code>dss-provisioner refresh [OPTIONS]\n</code></pre> Option Default Description <code>--auto-approve</code> <code>false</code> Skip confirmation prompt <p>Reads the current state of each tracked resource from DSS and updates the local state file. Useful after manual changes in the DSS UI.</p>"},{"location":"reference/cli/#drift","title":"<code>drift</code>","text":"<p>Show drift between state and the live DSS instance.</p> <pre><code>dss-provisioner drift [OPTIONS]\n</code></pre> <p>Read-only command that compares the state file against live DSS. Does not modify state. Shows which resources have drifted and what changed.</p>"},{"location":"reference/cli/#validate","title":"<code>validate</code>","text":"<p>Validate the configuration file.</p> <pre><code>dss-provisioner validate [OPTIONS]\n</code></pre> <p>Parses and validates the YAML configuration without connecting to DSS. Useful in CI pipelines or pre-commit hooks.</p>"},{"location":"reference/resources/","title":"Resource reference","text":"<p>Auto-generated API documentation for all resource types.</p>"},{"location":"reference/resources/#base","title":"Base","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource","title":"<code>dss_provisioner.resources.base.Resource</code>","text":"<p>Base class for all DSS resources.</p> <p>Resources are pure data - they define the desired state. Handlers know how to CRUD resources.</p>"},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.name","title":"<code>name = Field(pattern='^[a-zA-Z0-9_]+$')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.description","title":"<code>description = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.tags","title":"<code>tags = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.depends_on","title":"<code>depends_on = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.address","title":"<code>address</code>  <code>property</code>","text":"<p>Unique address for this resource (e.g., 'dss_dataset.my_dataset').</p>"},{"location":"reference/resources/#dss_provisioner.resources.base.Resource.reference_names","title":"<code>reference_names()</code>","text":"<p>Names of other resources this one references (auto-collected from Ref markers).</p>"},{"location":"reference/resources/#variables","title":"Variables","text":""},{"location":"reference/resources/#variablesresource","title":"VariablesResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.variables.VariablesResource","title":"<code>dss_provisioner.resources.variables.VariablesResource</code>","text":"<p>Project variables resource (singleton per project).</p> <p>Manages DSS project variables in two scopes:</p> <ul> <li><code>standard</code>: shared across all instances</li> <li><code>local</code>: instance-specific overrides</li> </ul>"},{"location":"reference/resources/#code-environments","title":"Code Environments","text":""},{"location":"reference/resources/#codeenvresource","title":"CodeEnvResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.code_env.CodeEnvResource","title":"<code>dss_provisioner.resources.code_env.CodeEnvResource</code>","text":"<p>Project default code environment resource (singleton per project).</p> <p>Selects existing instance-level code environments as the project defaults. Code environments themselves are not created or managed \u2014 only the project setting that points to them.</p>"},{"location":"reference/resources/#zones","title":"Zones","text":""},{"location":"reference/resources/#zoneresource","title":"ZoneResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.zone.ZoneResource","title":"<code>dss_provisioner.resources.zone.ZoneResource</code>","text":"<p>A DSS flow zone.</p> <p>Zones partition the flow graph into logical sections (e.g. raw, curated). They must be provisioned before datasets/recipes that reference them.</p>"},{"location":"reference/resources/#git-libraries","title":"Git Libraries","text":""},{"location":"reference/resources/#gitlibraryresource","title":"GitLibraryResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.git_library.GitLibraryResource","title":"<code>dss_provisioner.resources.git_library.GitLibraryResource</code>","text":"<p>Git library reference for a DSS project.</p> <p>Manages external Git repositories imported into the project library. Each entry maps to a Git reference in DSS's <code>external-libraries.json</code>.</p>"},{"location":"reference/resources/#managed-folders","title":"Managed Folders","text":""},{"location":"reference/resources/#managedfolderresource","title":"ManagedFolderResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.managed_folder.ManagedFolderResource","title":"<code>dss_provisioner.resources.managed_folder.ManagedFolderResource</code>","text":"<p>Base resource for DSS managed folders.</p>"},{"location":"reference/resources/#dss_provisioner.resources.managed_folder.ManagedFolderResource.to_dss_params","title":"<code>to_dss_params()</code>","text":"<p>Build the DSS API params dict from DSSParam-annotated fields.</p>"},{"location":"reference/resources/#filesystemmanagedfolderresource","title":"FilesystemManagedFolderResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.managed_folder.FilesystemManagedFolderResource","title":"<code>dss_provisioner.resources.managed_folder.FilesystemManagedFolderResource</code>","text":"<p>Filesystem-specific managed folder resource.</p>"},{"location":"reference/resources/#uploadmanagedfolderresource","title":"UploadManagedFolderResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.managed_folder.UploadManagedFolderResource","title":"<code>dss_provisioner.resources.managed_folder.UploadManagedFolderResource</code>","text":"<p>Upload-specific managed folder resource.</p>"},{"location":"reference/resources/#exposed-objects","title":"Exposed Objects","text":""},{"location":"reference/resources/#exposedobjectresource","title":"ExposedObjectResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.exposed_object.ExposedObjectResource","title":"<code>dss_provisioner.resources.exposed_object.ExposedObjectResource</code>","text":"<p>Base resource for DSS exposed objects.</p>"},{"location":"reference/resources/#exposeddatasetresource","title":"ExposedDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.exposed_object.ExposedDatasetResource","title":"<code>dss_provisioner.resources.exposed_object.ExposedDatasetResource</code>","text":"<p>Expose a local dataset to one or more target projects.</p>"},{"location":"reference/resources/#exposedmanagedfolderresource","title":"ExposedManagedFolderResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.exposed_object.ExposedManagedFolderResource","title":"<code>dss_provisioner.resources.exposed_object.ExposedManagedFolderResource</code>","text":"<p>Expose a local managed folder to one or more target projects.</p>"},{"location":"reference/resources/#foreign-objects","title":"Foreign Objects","text":""},{"location":"reference/resources/#foreigndatasetresource","title":"ForeignDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.foreign.ForeignDatasetResource","title":"<code>dss_provisioner.resources.foreign.ForeignDatasetResource</code>","text":"<p>A declared foreign dataset reference available to this project.</p>"},{"location":"reference/resources/#foreignmanagedfolderresource","title":"ForeignManagedFolderResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.foreign.ForeignManagedFolderResource","title":"<code>dss_provisioner.resources.foreign.ForeignManagedFolderResource</code>","text":"<p>A declared foreign managed folder reference available to this project.</p>"},{"location":"reference/resources/#datasets","title":"Datasets","text":""},{"location":"reference/resources/#datasetresource","title":"DatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.DatasetResource","title":"<code>dss_provisioner.resources.dataset.DatasetResource</code>","text":"<p>Base resource for DSS datasets.</p>"},{"location":"reference/resources/#dss_provisioner.resources.dataset.DatasetResource.to_dss_params","title":"<code>to_dss_params()</code>","text":"<p>Build the DSS API params dict from DSSParam-annotated fields.</p>"},{"location":"reference/resources/#snowflakedatasetresource","title":"SnowflakeDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.SnowflakeDatasetResource","title":"<code>dss_provisioner.resources.dataset.SnowflakeDatasetResource</code>","text":"<p>Snowflake-specific dataset resource.</p>"},{"location":"reference/resources/#oracledatasetresource","title":"OracleDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.OracleDatasetResource","title":"<code>dss_provisioner.resources.dataset.OracleDatasetResource</code>","text":"<p>Oracle-specific dataset resource.</p>"},{"location":"reference/resources/#filesystemdatasetresource","title":"FilesystemDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.FilesystemDatasetResource","title":"<code>dss_provisioner.resources.dataset.FilesystemDatasetResource</code>","text":"<p>Filesystem-specific dataset resource.</p>"},{"location":"reference/resources/#uploaddatasetresource","title":"UploadDatasetResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.UploadDatasetResource","title":"<code>dss_provisioner.resources.dataset.UploadDatasetResource</code>","text":"<p>Upload-specific dataset resource.</p>"},{"location":"reference/resources/#column","title":"Column","text":""},{"location":"reference/resources/#dss_provisioner.resources.dataset.Column","title":"<code>dss_provisioner.resources.dataset.Column</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A column in a dataset schema.</p>"},{"location":"reference/resources/#recipes","title":"Recipes","text":""},{"location":"reference/resources/#reciperesource","title":"RecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.RecipeResource","title":"<code>dss_provisioner.resources.recipe.RecipeResource</code>","text":"<p>Base resource for DSS recipes.</p>"},{"location":"reference/resources/#pythonreciperesource","title":"PythonRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.PythonRecipeResource","title":"<code>dss_provisioner.resources.recipe.PythonRecipeResource</code>","text":"<p>Python recipe resource.</p>"},{"location":"reference/resources/#sqlqueryreciperesource","title":"SQLQueryRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.SQLQueryRecipeResource","title":"<code>dss_provisioner.resources.recipe.SQLQueryRecipeResource</code>","text":"<p>SQL query recipe resource.</p>"},{"location":"reference/resources/#syncreciperesource","title":"SyncRecipeResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.recipe.SyncRecipeResource","title":"<code>dss_provisioner.resources.recipe.SyncRecipeResource</code>","text":"<p>Sync recipe resource.</p>"},{"location":"reference/resources/#scenarios","title":"Scenarios","text":""},{"location":"reference/resources/#scenarioresource","title":"ScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.ScenarioResource","title":"<code>dss_provisioner.resources.scenario.ScenarioResource</code>","text":"<p>Base resource for DSS scenarios.</p>"},{"location":"reference/resources/#stepbasedscenarioresource","title":"StepBasedScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.StepBasedScenarioResource","title":"<code>dss_provisioner.resources.scenario.StepBasedScenarioResource</code>","text":"<p>Step-based scenario with declarative steps.</p>"},{"location":"reference/resources/#pythonscenarioresource","title":"PythonScenarioResource","text":""},{"location":"reference/resources/#dss_provisioner.resources.scenario.PythonScenarioResource","title":"<code>dss_provisioner.resources.scenario.PythonScenarioResource</code>","text":"<p>Custom Python scenario.</p>"},{"location":"reference/resources/#modules","title":"Modules","text":""},{"location":"reference/resources/#modulespec","title":"ModuleSpec","text":""},{"location":"reference/resources/#dss_provisioner.config.modules.ModuleSpec","title":"<code>dss_provisioner.config.modules.ModuleSpec</code>","text":"<p>Specification for a single module invocation in the YAML config.</p>"},{"location":"reference/resources/#dss_provisioner.config.modules.ModuleSpec.invocations","title":"<code>invocations()</code>","text":"<p>Return <code>(kwargs, label)</code> pairs for each call to make.</p>"},{"location":"reference/resources/#moduleexpansionerror","title":"ModuleExpansionError","text":""},{"location":"reference/resources/#dss_provisioner.config.modules.ModuleExpansionError","title":"<code>dss_provisioner.config.modules.ModuleExpansionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when module resolution or expansion fails.</p>"},{"location":"reference/resources/#expand_modules","title":"expand_modules","text":""},{"location":"reference/resources/#dss_provisioner.config.modules.expand_modules","title":"<code>dss_provisioner.config.modules.expand_modules(modules, config_dir)</code>","text":"<p>Expand all module specs into a flat list of resources.</p>"}]}